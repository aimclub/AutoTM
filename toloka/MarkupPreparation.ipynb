{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import artm\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from dataclasses import dataclass, astuple\n",
    "from typing import Dict, List, Tuple, Iterator, Optional\n",
    "import functools\n",
    "import itertools\n",
    "import tqdm\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "\n",
    "MAX_WORDS_PER_TOPIC = 20\n",
    "MAX_TEXT_LENGTH = 150\n",
    "BASE_PATH = \"models/experiments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func: Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets:\n",
    "    datasets = {\n",
    "        \"lentaru\": os.path.join('datasets', 'lenta_ru','lenta-ru-news_sample_10000_processed.csv'),\n",
    "    }\n",
    "    \n",
    "    loaded_datasets = dict()\n",
    "    \n",
    "    @classmethod\n",
    "    def dataset(cls, name: str) -> pd.DataFrame:\n",
    "        data = cls.loaded_datasets.get(name, pd.read_csv(cls.datasets[name]))\n",
    "        return data\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TmModel:\n",
    "    exp_id: str\n",
    "    model_id: str\n",
    "    dataset_name: str\n",
    "    data: pd.DataFrame\n",
    "    model: object\n",
    "    topic_word_dist: np.ndarray\n",
    "    dtd: np.ndarray\n",
    "    topics_dict: Dict[str, List[str]]\n",
    "        \n",
    "    def __iter__(self):\n",
    "#         return iter((data, model, topic_word_dist, dtd, topics_dict))\n",
    "        return iter(astuple(self))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"TM_{self.exp_id}_{self.model_id}_{self.dataset_name}\"\n",
    "\n",
    "\n",
    "# def read_data(exp_path: str) -> TmModel:\n",
    "#     _, num = os.path.basename(p).split('_')[-1]\n",
    "\n",
    "#     dataset_path = os.path.join('datasets', 'lenta_ru','lenta-ru-news_sample_10000_processed.csv')\n",
    "#     model_path_pattern = os.path.join(exp_path, f'best_model/model_{num}_*')\n",
    "    \n",
    "\n",
    "#     phi_path = os.path.join(exp_path, 'matrixes/phi.npy')\n",
    "#     theta_path =  os.path.join(exp_path, 'matrixes/theta.npy')\n",
    "\n",
    "#     data = pd.read_csv(dataset_path)\n",
    "\n",
    "#     model = artm.load_artm_model(model_path)\n",
    "\n",
    "#     topic_word_dist = np.load(phi_path)\n",
    "#     doc_topic_dist = np.load(theta_path)\n",
    "\n",
    "#     topics_dict = model.score_tracker['TopTokensScore'].last_tokens\n",
    "    \n",
    "#     return TmModel(data, model, topic_word_dist, doc_topic_dist.T, topics_dict)\n",
    "\n",
    "def read_exp_data(exp_path: str, model_ids: Optional[List[str]] = None) -> Iterator[TmModel]:\n",
    "    exp_name = os.path.basename(exp_path)\n",
    "    \n",
    "    # Example: experiment_<dataset>_S_<topic_count>_<exp_id>\n",
    "    _, dataset_name, _, topic_count, exp_id = exp_name.split('_')\n",
    "    \n",
    "    data = Datasets.dataset(dataset_name)\n",
    "\n",
    "    model_path_pattern = os.path.join(exp_path, f'best_model/model_{exp_id}_*')\n",
    "    model_paths = glob.glob(model_path_pattern)\n",
    "\n",
    "    for model_path in model_paths:\n",
    "        model_id = model_path.split('_')[-1]\n",
    "        \n",
    "        if model_ids is not None and model_id not in model_ids:\n",
    "            continue\n",
    "        \n",
    "        metrics_path = os.path.join(exp_path, f'metrics/saved_metrics_{model_id}.pkl')\n",
    "        phi_path = os.path.join(exp_path, f'matrixes/phi_{model_id}.npy')\n",
    "        theta_path =  os.path.join(exp_path, f'matrixes/theta_{model_id}.npy')\n",
    "\n",
    "        if not all(os.path.exists(path) for path in [metrics_path, phi_path, theta_path]):\n",
    "            continue\n",
    "        \n",
    "        with open(metrics_path, 'rb') as f:\n",
    "            metrics = pickle.load(f)\n",
    "        \n",
    "        if not metrics['all_topics']:\n",
    "            continue\n",
    "        \n",
    "        model = artm.load_artm_model(model_path)\n",
    "        topic_word_dist = np.load(phi_path)\n",
    "        doc_topic_dist = np.load(theta_path)\n",
    "        topics_dict = model.score_tracker['TopTokensScore'].last_tokens\n",
    "\n",
    "        yield TmModel(exp_id, model_id, dataset_name, data, model, topic_word_dist, doc_topic_dist.T, topics_dict)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func: auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_align_topics(max_topics: int):\n",
    "    def align_topics(topics):\n",
    "        el = np.zeros(max_topics)\n",
    "        el[:len(topics)] = topics\n",
    "        el[:max_topics - len(topics)] = np.nan\n",
    "        return el\n",
    "    return align_topics\n",
    "\n",
    "def parse_topic_name(name):\n",
    "    return int(name[len('main'):]) if name.startswith('main') else int(name[len('back'):])\n",
    "\n",
    "def validate_text_samples(df: pd.DataFrame, tm: TmModel) -> None:\n",
    "    s = df.apply(lambda x: len(sum(x, start=[])) == len(set(el for l in x for el in l)), axis=1)\n",
    "        \n",
    "    assert all(s), str(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func: Making task №1 and №2 of the markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_topics(tm: TmModel) -> pd.DataFrame:\n",
    "    df = pd.DataFrame([{'exp_id': tm.exp_id, 'model_id': tm.model_id, 'dataset_name': tm.dataset_name, \n",
    "                        'topic_id': topic_id, 'wordset': ' '.join(words[:MAX_WORDS_PER_TOPIC])} for topic_id, words in tm.topics_dict.items() if topic_id.startswith('main')])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func: Making task №3 of the markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples_topic2word(tm: TmModel) -> pd.DataFrame:\n",
    "    exp_id, model_id, dataset_name, data, model, topic_word_dist, dtd, topics_dict = tm\n",
    "\n",
    "    # prepare auxillary structures\n",
    "    main_topics_count = max(int(k[len('main'):]) for k in topics_dict.keys() if k.startswith('main')) + 1\n",
    "    back_topics_count = max((int(k[len('back'):]) for k in topics_dict.keys() if k.startswith('back')), default=-1) + 1\n",
    "    topic_num2words = {int(k[len('main'):]) if k.startswith('main') else main_topics_count + int(k[len('back'):]): v[:20] for k,v in topics_dict.items()}\n",
    "\n",
    "    dtd = dtd[:, :main_topics_count]\n",
    "\n",
    "    phi_df = model.get_phi()\n",
    "\n",
    "\n",
    "    top_topics = np.flip(np.argsort(dtd, axis=1)).astype('int')[:, :2]\n",
    "\n",
    "    # prepare tfidf to be used for word choosing\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(data['processed_text'])\n",
    "    feat_names = vectorizer.get_feature_names_out()\n",
    "    rows, cols = X.nonzero()\n",
    "\n",
    "    def get_words_by_tfidf(doc_num):\n",
    "        columns = cols[np.where(rows == doc_num)]\n",
    "        arr = np.flip(np.argsort(X[doc_num, columns].toarray().reshape(-1)))\n",
    "        words_by_tfidf = [feat_names[cols[idx]] for idx in arr[:20]]\n",
    "        return words_by_tfidf\n",
    "\n",
    "\n",
    "    doc2words_by_tfidf = [get_words_by_tfidf(doc_num) for doc_num in range(data.shape[0])]\n",
    "\n",
    "\n",
    "    # structures to select topics relative to a chosen word\n",
    "    word2probtopic = np.argsort(phi_df * -1, axis=1)\n",
    "\n",
    "    topics_by_popularity = np.argsort(np.mean(dtd, axis=0))\n",
    "    word2most_probable_topics_df = np.argsort(phi_df * -1, axis=1).iloc[:, :10]\n",
    "    word2poptopic = word2most_probable_topics_df.apply(lambda x: [el for el in topics_by_popularity if el in set(x)][:10], axis=1)\n",
    "\n",
    "\n",
    "    def choose_word_and_topics(doc_id: int, text: str, tpcs: List[int]) -> Tuple[str, List[int]]:\n",
    "        already_chosen_tpcs = set(tpcs)\n",
    "\n",
    "    #     # 1. way of choosing\n",
    "    #     word_sets = (\n",
    "    #         w for t in tpcs \n",
    "    #         for w in np.flip(phi_df[f'main{t}'].argsort())[:20].index.tolist()\n",
    "    #     )\n",
    "\n",
    "    #     word_set = list(set(word_sets))\n",
    "    #     np.random.shuffle(word_set)\n",
    "\n",
    "    #     # problem with StopIteration\n",
    "    #     word = next(w for w in word_set)\n",
    "        # 2. TF-IDF\n",
    "        word = np.random.choice(doc2words_by_tfidf[doc_id][:10], size=1)[0]\n",
    "\n",
    "        # 3. random word from the text\n",
    "    #     word = np.random.choice(text.split(' '), size=1)[0]\n",
    "\n",
    "        # finding the most probable topics this word belongs to\n",
    "        # exclude tpcs\n",
    "        most_prob_topics = [w for w in word2probtopic.loc[word] if w not in already_chosen_tpcs][:2]\n",
    "        already_chosen_tpcs.update(most_prob_topics)\n",
    "\n",
    "        # two most popular topics this word is among top 10 words of \n",
    "        # exclude tpcs\n",
    "        most_pop_topics = [w for w in word2poptopic.loc[word] if w not in already_chosen_tpcs][:2]\n",
    "        already_chosen_tpcs.update(most_pop_topics)\n",
    "\n",
    "        # exclude tpcs\n",
    "        rest_topics = [w for w in word2poptopic.loc[word] if w not in already_chosen_tpcs]\n",
    "        \n",
    "        random_topics = np.random.choice(rest_topics, size=1)[0]\n",
    "        already_chosen_tpcs.add(random_topics)\n",
    "        \n",
    "        all_topics = [*tpcs, *most_prob_topics, *most_pop_topics, random_topics]\n",
    "        \n",
    "        assert len(all_topics) == len(set(all_topics)), tm\n",
    "\n",
    "        return {\n",
    "            \"doc\": doc_id,\n",
    "            \"text\": text,\n",
    "            \"chosen_word\": word,\n",
    "            \"top_topics_for_text\": tpcs,\n",
    "            \"most_prob_topics_for_word\": most_prob_topics,\n",
    "            \"most_pop_topics_for_word\": most_pop_topics,\n",
    "            \"random_topics\": random_topics,\n",
    "            \"all_topics\": all_topics\n",
    "        }\n",
    "\n",
    "    text2top_topics = list(enumerate(zip(data['processed_text'], top_topics)))\n",
    "\n",
    "#     highlighted_word_df = pd.DataFrame([choose_word_and_topics(i, text, tpcs) for i, (text, tpcs) in tqdm.tqdm(text2top_topics)])\n",
    "    highlighted_word_df = pd.DataFrame([choose_word_and_topics(i, text, tpcs) for i, (text, tpcs) in text2top_topics])\n",
    "\n",
    "    max_topics = highlighted_word_df['all_topics'].map(len).max()\n",
    "    highlighted_word_df[[f'topic_{i}' for i in range(max_topics)]] = highlighted_word_df['all_topics'].apply(make_align_topics(max_topics)).tolist()\n",
    "    for c in [f'topic_{i}' for i in range(max_topics)]:\n",
    "        highlighted_word_df[c] = highlighted_word_df[c].apply(lambda tnum: topic_num2words[tnum])\n",
    "\n",
    "    highlighted_word_df[['exp_id', 'model_id', 'dataset_name']] = exp_id, model_id, dataset_name\n",
    "        \n",
    "    return highlighted_word_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func: Making task №4 of the markup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples_topic2text(tm: TmModel) -> pd.DataFrame:\n",
    "    exp_id, model_id, dataset_name, data, model, topic_word_dist, dtd, topics_dict = tm\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    MAX_TOPIC_SAMPLES = 5\n",
    "\n",
    "    main_topics_count = max(int(k[len('main'):]) for k in topics_dict.keys() if k.startswith('main')) + 1\n",
    "    back_topics_count = max((int(k[len('back'):]) for k in topics_dict.keys() if k.startswith('back')), default=-1) + 1\n",
    "    topic_num2words = {int(k[len('main'):]) if k.startswith('main') else main_topics_count + int(k[len('back'):]): v[:20] for k,v in topics_dict.items()}\n",
    "\n",
    "    dtd = dtd[:, :main_topics_count]\n",
    "    argsrt_dtd = np.argsort(dtd, axis=1)\n",
    "    top_topics = np.flip(argsrt_dtd).astype('int')\n",
    "    \n",
    "    def generate_options(i, top_topics_record):\n",
    "        top_topics_row = [c_idx for c_idx in top_topics_record[:3] if dtd[i, c_idx] > 0.01]\n",
    "#         random_topics = rng.choice(argsrt_dtd[i, :main_topics_count - len(top_topics_row)], size=MAX_TOPIC_SAMPLES - len(top_topics_row), shuffle=True, replace=False).tolist()\n",
    "        random_topics = rng.choice(top_topics_record[3:], size=MAX_TOPIC_SAMPLES - len(top_topics_row), shuffle=True, replace=False).tolist()\n",
    "\n",
    "        return {\n",
    "            \"doc\": i,\n",
    "            \"topics\": top_topics_row,\n",
    "            \"random_topics\": random_topics\n",
    "        }\n",
    "    \n",
    "\n",
    "    doc2topics = pd.DataFrame([generate_options(i, topics) for i, topics in enumerate(top_topics)])\n",
    "    doc2topics = doc2topics.set_index('doc')\n",
    "\n",
    "    markup_df = data.merge(doc2topics, left_index=True, right_index=True)\n",
    "    markup_df = markup_df[['text', 'tokens_len', 'topics', 'random_topics']]\n",
    "    markup_df = markup_df[markup_df['tokens_len'] <= MAX_TEXT_LENGTH]\n",
    "    markup_df[[f'topic_{i}' for i in range(MAX_TOPIC_SAMPLES)]] = (markup_df['topics'] + markup_df['random_topics']).apply(make_align_topics(MAX_TOPIC_SAMPLES)).tolist()\n",
    "    markup_df[[f'topic_{i}' for i in range(MAX_TOPIC_SAMPLES)]] = markup_df[[f'topic_{i}' for i in range(MAX_TOPIC_SAMPLES)]].astype('int')\n",
    "    \n",
    "    for c in [f'topic_{i}' for i in range(MAX_TOPIC_SAMPLES)]:\n",
    "        markup_df[c] = markup_df[c].apply(lambda tnum: topic_num2words[tnum])\n",
    "    \n",
    "    markup_df[['exp_id', 'model_id', 'dataset_name']] = exp_id, model_id, dataset_name\n",
    "    \n",
    "    validate_text_samples(markup_df[['topics', 'random_topics']], tm)\n",
    "    \n",
    "    return markup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_for_tasks(base_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    topics_dfs, topic2text_dfs, topic2word_dfs = [], [], [] \n",
    "\n",
    "    for exp_path in tqdm.tqdm(glob.glob(os.path.join(base_path, \"experiment_*\")), desc=\"experiment\"):\n",
    "        models = read_exp_data(exp_path) \n",
    "        for tm in tqdm.tqdm(models, desc=\"models\"):\n",
    "            topics_dfs.append(make_sample_topics(tm))\n",
    "            topic2word_dfs.append(make_samples_topic2word(tm))\n",
    "            topic2text_dfs.append(make_samples_topic2text(tm))\n",
    "\n",
    "    topics_df = pd.concat(topics_dfs)\n",
    "    topic2word_df = pd.concat(topic2word_dfs)\n",
    "    topic2text_df = pd.concat(topic2text_dfs)\n",
    "\n",
    "    return topics_df, topic2word_df, topic2text_df\n",
    "\n",
    "# save samples for labeling in the json formath required by Toloka \n",
    "def write_topics_to_json(topics_df: pd.DataFrame):\n",
    "    topics_df[\"correct_bad_words\"] = None\n",
    "    cols_to_rename ={c:f\"INPUT:{c}\" for c in topics_df.columns.tolist()}\n",
    "    \n",
    "    toloka_input_topics_df = topics_df.rename(columns=cols_to_rename)\n",
    "    \n",
    "    with open(\"toloka_input_topics.json\", 'w') as f:\n",
    "        records = toloka_input_topics_df.to_dict('records')\n",
    "        json.dump(records, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiment:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "models: 0it [00:00, ?it/s]\u001b[A\n",
      "models: 1it [00:17, 17.75s/it]\u001b[A\n",
      "experiment:  25%|██▌       | 1/4 [00:17<00:53, 17.75s/it]\n",
      "models: 0it [00:00, ?it/s]\u001b[A\n",
      "models: 1it [00:19, 19.31s/it]\u001b[A\n",
      "models: 2it [00:34, 16.87s/it]\u001b[A\n",
      "models: 3it [00:49, 16.06s/it]\u001b[A\n",
      "models: 4it [01:05, 15.87s/it]\u001b[A\n",
      "models: 5it [01:23, 16.92s/it]\u001b[A\n",
      "models: 6it [01:40, 16.69s/it]\u001b[A\n",
      "experiment:  50%|█████     | 2/4 [01:57<02:12, 66.23s/it]\n",
      "models: 0it [00:00, ?it/s]\u001b[A\n",
      "models: 1it [00:17, 17.07s/it]\u001b[A\n",
      "experiment:  75%|███████▌  | 3/4 [02:14<00:43, 43.78s/it]\n",
      "models: 0it [00:00, ?it/s]\u001b[A\n",
      "models: 1it [00:15, 15.92s/it]\u001b[A\n",
      "experiment: 100%|██████████| 4/4 [02:30<00:00, 37.73s/it]\n"
     ]
    }
   ],
   "source": [
    "topics_df, topic2word_df, topic2text_df = generate_samples_for_tasks(BASE_PATH)\n",
    "\n",
    "topics_df.to_parquet(\"topics_samples.parquet\")\n",
    "topic2word_df.to_parquet(\"topic2word_samples.parquet\")\n",
    "topic2text_df.to_parquet(\"topic2text_samples.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_topics_to_json(topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_id</th>\n",
       "      <th>model_id</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>wordset</th>\n",
       "      <th>correct_bad_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1655995996</td>\n",
       "      <td>1655996024</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main0</td>\n",
       "      <td>который компания также свой мочь новый процент...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1655995996</td>\n",
       "      <td>1655996024</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main1</td>\n",
       "      <td>матч команда сборная россия который чемпионат ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1655995996</td>\n",
       "      <td>1655996024</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main2</td>\n",
       "      <td>ахмедова узбекский прививка врач судебный фото...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1655995996</td>\n",
       "      <td>1655996024</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main3</td>\n",
       "      <td>грузия грузинский военный осетия южный конфлик...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1655995996</td>\n",
       "      <td>1655996024</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main4</td>\n",
       "      <td>нефть новый нефтяной расход газпром госпрограм...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1656001268</td>\n",
       "      <td>1656001315</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main5</td>\n",
       "      <td>дело сотрудник сообщать область задерживать уг...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1656001268</td>\n",
       "      <td>1656001315</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main6</td>\n",
       "      <td>президент украина глава заявлять партия минист...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1656001268</td>\n",
       "      <td>1656001315</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main7</td>\n",
       "      <td>процент рубль доллар составлять тысяча миллиар...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1656001268</td>\n",
       "      <td>1656001315</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main8</td>\n",
       "      <td>который проект миллион фильм новый тысяча такж...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1656001268</td>\n",
       "      <td>1656001315</td>\n",
       "      <td>lentaru</td>\n",
       "      <td>main9</td>\n",
       "      <td>человек сообщать происходить город находиться ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        exp_id    model_id dataset_name topic_id  \\\n",
       "0   1655995996  1655996024      lentaru    main0   \n",
       "1   1655995996  1655996024      lentaru    main1   \n",
       "2   1655995996  1655996024      lentaru    main2   \n",
       "3   1655995996  1655996024      lentaru    main3   \n",
       "4   1655995996  1655996024      lentaru    main4   \n",
       "..         ...         ...          ...      ...   \n",
       "5   1656001268  1656001315      lentaru    main5   \n",
       "6   1656001268  1656001315      lentaru    main6   \n",
       "7   1656001268  1656001315      lentaru    main7   \n",
       "8   1656001268  1656001315      lentaru    main8   \n",
       "9   1656001268  1656001315      lentaru    main9   \n",
       "\n",
       "                                              wordset correct_bad_words  \n",
       "0   который компания также свой мочь новый процент...              None  \n",
       "1   матч команда сборная россия который чемпионат ...              None  \n",
       "2   ахмедова узбекский прививка врач судебный фото...              None  \n",
       "3   грузия грузинский военный осетия южный конфлик...              None  \n",
       "4   нефть новый нефтяной расход газпром госпрограм...              None  \n",
       "..                                                ...               ...  \n",
       "5   дело сотрудник сообщать область задерживать уг...              None  \n",
       "6   президент украина глава заявлять партия минист...              None  \n",
       "7   процент рубль доллар составлять тысяча миллиар...              None  \n",
       "8   который проект миллион фильм новый тысяча такж...              None  \n",
       "9   человек сообщать происходить город находиться ...              None  \n",
       "\n",
       "[90 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff = (np.max(tm.dtd, axis=1) - np.min(tm.dtd, axis=1))\n",
    "\n",
    "# px.histogram(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# px.box(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
