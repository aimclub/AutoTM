{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 826 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pymystem3\n",
    "from pyspark.sql.types import BooleanType, IntegerType, StringType, FloatType, ArrayType\n",
    "from pyspark.sql.functions import dayofyear, weekofyear, to_date, concat_ws, collect_list, year, udf, col, size, countDistinct, lit, sum, expr \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "import re\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import artm\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyarrow import parquet\n",
    "\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 412 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "full_datasets_path = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 396 µs\n"
     ]
    }
   ],
   "source": [
    "dataset_names = [\"20newsgroups\",\n",
    "                 \"amazon_food\",\n",
    "                 \"hotel-reviews\",\n",
    "                 \"lenta_ru\",\n",
    "                 \"ads_data\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_NEW/20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/test_set_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/socks.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Callable\n",
      "Installing mystem to /home/jovyan/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.87 s\n"
     ]
    }
   ],
   "source": [
    "# dataset cleaning \n",
    "r_num = re.compile(r'([0-9]+)')\n",
    "r_punct = re.compile(r'[.\"\\[\\]/,()!?;:*#|\\\\%^$&{}~_`=-@]')\n",
    "r_white_space = re.compile(r'\\s{2,}')\n",
    "r_words = re.compile(r'\\W+')\n",
    "re1 = re.compile(r'  +')\n",
    "url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "r_rus = re.compile(r'[а-яА-Я]\\w+')\n",
    "r_html = re.compile(r'(\\<[^>]*\\>)')\n",
    "r_email = re.compile(r\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\")\n",
    "\n",
    "stop = stopwords.words(\"russian\") + ['который', 'это', 'весьма', 'вполне', 'наверное']\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "def process_punkt(text):\n",
    "    text = r_punct.sub(\" \", text)\n",
    "    text = r_vk_ids.sub(\" \", text)\n",
    "    text = r_num.sub(\" \", text)\n",
    "    text = r_white_space.sub(\" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def new_html(text):\n",
    "    text = r_html.sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_text_ru(text):\n",
    "\n",
    "    try:\n",
    "        text = new_html(text)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "    text = text.lower()\n",
    "    text = process_punkt(text)\n",
    "    text = re.findall(r_rus, text)\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    try:\n",
    "        tokens = r_words.split(text)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "    tokens = (x for x in tokens if len(x) >= 2 and not x.isdigit())\n",
    "    text = ' '.join(tokens)\n",
    "    tokens = m.lemmatize(text)\n",
    "    tokens = (x for x in tokens if x not in stop)\n",
    "    tokens = (x for x in tokens if x.isalpha())\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "def lemmatize_text_en(line):\n",
    "    try:\n",
    "        line = new_html(line)\n",
    "    except:\n",
    "        return ''\n",
    "    lmtzr = WordNetLemmatizer()    \n",
    "    stop = stopwords.words('english')\n",
    "    text_token = CountVectorizer().build_tokenizer()(line.lower())\n",
    "    text_token = (x for x in text_token if len(x) > 2 and not x.isdigit())\n",
    "    text_rmstop = (i for i in text_token if i not in stop)\n",
    "    text_rmstop = (x for x in text_rmstop if x.isalpha())\n",
    "    text_stem = ' '.join([lmtzr.lemmatize(w) for w in text_rmstop])\n",
    "    #stext_stem = remove_more_html(text_stem)\n",
    "    return text_stem\n",
    "    \n",
    "\n",
    "def text_to_tokens(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def remove_more_html(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('<', ' ').replace('>', ' ').replace('#36;', '$').replace(\n",
    "        '\\\\n', \"\\n\").replace('quot;', \"'\").replace('<br />', \"\\n\").replace(\n",
    "        '\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ').replace('img', ' ').replace('class', ' ').replace(\n",
    "        'src', ' ').replace('alt', ' ').replace('email', ' ').replace('icq', ' ').replace(\n",
    "        'href', ' ').replace('mem', ' ').replace('link', ' ').replace('mention', ' ').replace(\n",
    "        'onclick', ' ').replace('icq', ' ').replace('onmouseover', ' ').replace(\n",
    "        'local', ' ').replace('key', ' ').replace('target', ' ').replace('amp', ' ').replace(\n",
    "        'section', ' ').replace('search', ' ').replace('css', ' ').replace('style', ' ').replace(\n",
    "        'cc', ' ').replace('text', ' ').replace(\"img\", ' ').replace(\"expand\", ' ').replace(\n",
    "        \"text\", ' ').replace('\\n', ' ').replace('dnum', ' ').replace('xnum', ' ').replace(\n",
    "        'nnum', ' ').replace(\"ubuntu\", ' ').replace('server', ' ').replace('port', ' ').replace('nntp', ' ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def clear_url(text):\n",
    "    return re.sub(url_pattern, ' ', text)\n",
    "\n",
    "\n",
    "def tokens_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def get_rid_of_num(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [i for i in tokens if i != 'num']\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.06 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset(dataset_path, save_dataset_path, column_name, language):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    p = mp.Pool(mp.cpu_count())\n",
    "    if language == \"ru\":\n",
    "        dataset['processed_text'] = p.map(lemmatize_text_ru, dataset[column_name])\n",
    "    if language == 'en':\n",
    "        dataset['processed_text'] = p.map(lemmatize_text_en, dataset[column_name])\n",
    "    dataset['tokens_count'] = p.map(tokens_count, dataset['processed_text'])\n",
    "    \n",
    "    print('Initial dataset shape: ', dataset.shape)\n",
    "    dataset = dataset[dataset['tokens_count'] >= 3]\n",
    "    print('Filtered dataset shape: ', dataset.shape)\n",
    "    \n",
    "    dataset.to_csv(save_dataset_path, index=None)\n",
    "    print('Dataset is saved')\n",
    "    \n",
    "    return\n",
    "    \n",
    "\n",
    "def get_words_dict(text, stop_list):\n",
    "    \n",
    "    all_words = text\n",
    "    words = sorted(set(all_words)-stop_list)\n",
    "    \n",
    "    return {w: all_words.count(w) for w in words}\n",
    "\n",
    "def return_string_part(name_type, text):\n",
    "    \n",
    "    tokens = text.split()\n",
    "    tokens = [item for item in tokens if item != '']\n",
    "    tokens_dict = get_words_dict(tokens, set())\n",
    "    \n",
    "    return \" |\" + name_type + ' ' + ' '.join(['{}:{}'.format(k, v) for k, v in tokens_dict.items()])\n",
    "\n",
    "def prepare_voc(batches_dir, vw_path, data_path, column_name='processed_text'):\n",
    "    \n",
    "    print('Starting...')\n",
    "    with open(vw_path, 'w', encoding='utf8') as ofile:\n",
    "        num_parts = 0\n",
    "        try:\n",
    "            for file in os.listdir(data_path):\n",
    "                if file.startswith('part'):\n",
    "                    print('part_{}'.format(num_parts), end='\\r')\n",
    "                    if file.split('.')[-1] == 'csv':\n",
    "                        part=pd.read_csv(os.path.join(data_path, file))\n",
    "                    else:\n",
    "                        part = pd.read_parquet(os.path.join(data_path, file))\n",
    "                    part_processed = part[column_name].tolist()\n",
    "                    for text in part_processed:\n",
    "                        result = return_string_part('@default_class', text)\n",
    "                        ofile.write(result + '\\n')\n",
    "                    num_parts += 1\n",
    "\n",
    "        except NotADirectoryError:\n",
    "            print('part 1/1')\n",
    "            part = pd.read_csv(data_path)\n",
    "            part_processed = part[column_name].tolist()\n",
    "            for text in part_processed:\n",
    "                result = return_string_part('@default_class', text)\n",
    "                ofile.write(result + '\\n')\n",
    "\n",
    "    print(' batches {} \\n vocabulary {} \\n are ready'.format(batches_dir, vw_path))\n",
    "    \n",
    "def prepare_batch_vectorizer(batches_dir, vw_path, data_path, column_name='processed_text'):\n",
    "#     if not glob.glob(os.path.join(batches_dir, \"*\")):\n",
    "    prepare_voc(batches_dir, vw_path, data_path, column_name=column_name)\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_path=vw_path,\n",
    "        data_format=\"vowpal_wabbit\", \n",
    "        target_folder=batches_dir,\n",
    "    batch_size=100)\n",
    "#     else:\n",
    "#         batch_vectorizer = artm.BatchVectorizer(data_path=batches_dir, data_format='batches')\n",
    "        \n",
    "    return batch_vectorizer\n",
    "\n",
    "def vocab_preparation(VOCAB_PATH, DICTIONARY_PATH):\n",
    "    \n",
    "    if not os.path.exists(VOCAB_PATH):\n",
    "        with open(DICTIONARY_PATH, 'r') as dictionary_file:\n",
    "            with open(VOCAB_PATH, 'w') as vocab_file:\n",
    "                dictionary_file.readline()\n",
    "                dictionary_file.readline()\n",
    "                for line in dictionary_file:\n",
    "                    elems = re.split(', ', line)\n",
    "                    vocab_file.write(' '.join(elems[:2]) + '\\n')\n",
    "\n",
    "                    \n",
    "def prepearing_cooc_dict(BATCHES_DIR, WV_PATH, VOCAB_PATH, COOC_DICTIONARY_PATH, \n",
    "                         cooc_file_path_tf, cooc_file_path_df, ppmi_dict_tf, ppmi_dict_df):\n",
    "    print(f'BATCHES_DIR: {BATCHES_DIR}')\n",
    "    print(f'WV_PATH: {WV_PATH}')\n",
    "    print(f'VOCAB_PATH: {VOCAB_PATH}')\n",
    "    print(f'COOC_DICTIONARY_PATH: {COOC_DICTIONARY_PATH}')\n",
    "    print(f'cooc_file_path_tf: {cooc_file_path_tf}')\n",
    "    print(f'cooc_file_path_df: {cooc_file_path_df}')\n",
    "    print(f'ppmi_dict_tf: {ppmi_dict_tf}')\n",
    "    print(f'ppmi_dict_df: {ppmi_dict_df}')\n",
    "    \n",
    "    ! bigartm -v $VOCAB_PATH -c $WV_PATH --cooc-window 10 --write-cooc-tf $cooc_file_path_tf --write-cooc-df $cooc_file_path_df --write-ppmi-tf $ppmi_dict_tf --write-ppmi-df $ppmi_dict_df\n",
    "    \n",
    "    cooc_dict = artm.Dictionary()\n",
    "    cooc_dict.gather(\n",
    "        data_path=BATCHES_DIR, \n",
    "        cooc_file_path=ppmi_dict_tf,\n",
    "        vocab_file_path=VOCAB_PATH,\n",
    "        symmetric_cooc_values=True)\n",
    "    cooc_dict.save_text(COOC_DICTIONARY_PATH)\n",
    "                    \n",
    "        \n",
    "def prepare_all(DATASET_PATH, column_name, dataset_lang):\n",
    "    \n",
    "    SAVE_DATASET_PATH = os.path.join(DATASET_PATH, 'processed_dataset.csv')\n",
    "    BATCHES_DIR = os.path.join(DATASET_PATH, 'batches')\n",
    "    WV_PATH = os.path.join(DATASET_PATH, 'wv.txt')\n",
    "    DOCUMENTS_TO_BATCH_PATH = os.path.join(DATASET_PATH, 'dataset.csv')\n",
    "    DICTIONARY_PATH = os.path.join(DATASET_PATH, 'dictionary.txt')\n",
    "    VOCAB_PATH = os.path.join(DATASET_PATH, 'vocab.txt')\n",
    "    COOC_DICTIONARY_PATH = os.path.join(DATASET_PATH, 'cooc_dictionary.txt')\n",
    "    cooc_file_path_tf = os.path.join(DATASET_PATH, 'cooc_tf.txt')\n",
    "    cooc_file_path_df = os.path.join(DATASET_PATH, 'cooc_df.txt')\n",
    "    ppmi_dict_tf = os.path.join(DATASET_PATH,'ppmi_tf.txt')\n",
    "    ppmi_dict_df = os.path.join(DATASET_PATH, 'ppmi_df.txt')\n",
    "    \n",
    "    clean_dataset(DOCUMENTS_TO_BATCH_PATH, SAVE_DATASET_PATH, column_name, dataset_lang)\n",
    "    \n",
    "    batch_vectorizer = prepare_batch_vectorizer(BATCHES_DIR, WV_PATH, DOCUMENTS_TO_BATCH_PATH)\n",
    "    print('Batches are ready')\n",
    "    \n",
    "    my_dictionary = artm.Dictionary()\n",
    "    my_dictionary.gather(data_path=BATCHES_DIR, vocab_file_path=WV_PATH)\n",
    "    my_dictionary.filter(min_df=3, max_df=0.95, class_id='text')\n",
    "    my_dictionary.save_text(DICTIONARY_PATH)\n",
    "    print('Dictionary is ready')\n",
    "    \n",
    "    vocab_preparation(VOCAB_PATH, DICTIONARY_PATH)\n",
    "    prepearing_cooc_dict(BATCHES_DIR, WV_PATH, VOCAB_PATH, \n",
    "                         COOC_DICTIONARY_PATH, cooc_file_path_tf, \n",
    "                         cooc_file_path_df, ppmi_dict_tf, \n",
    "                         ppmi_dict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Newsgroups full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews.csv\t\t      cooc_dictionary.txt   ppmi_df.txt\n",
      "Reviews_10000_lemmatized.csv  cooc_tf.txt\t    ppmi_tf.txt\n",
      "batches\t\t\t      dictionary.txt\t    test_set_data_voc.txt\n",
      "cooc_df.txt\t\t      mutual_info_dict.pkl  vocab.txt\n",
      "time: 480 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/amazon_food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 430 ms\n"
     ]
    }
   ],
   "source": [
    "newsgroups = pd.read_csv('/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/20newsgroups/original_dataset/prepared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/20newsgroups/original_dataset/prepared.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/prepared.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 318 µs\n"
     ]
    }
   ],
   "source": [
    "NG_DATASET = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 382 µs\n"
     ]
    }
   ],
   "source": [
    "ng_column_name = 'processed_text'\n",
    "dataset_lang = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape:  (11299, 5)\n",
      "Filtered dataset shape:  (11280, 5)\n",
      "Dataset is saved\n",
      "Starting...\n",
      "part 1/1\n",
      " batches /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/batches \n",
      " vocabulary /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/wv.txt \n",
      " are ready\n",
      "Batches are ready\n",
      "Dictionary is ready\n",
      "BATCHES_DIR: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/batches\n",
      "WV_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/wv.txt\n",
      "VOCAB_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/vocab.txt\n",
      "COOC_DICTIONARY_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/cooc_dictionary.txt\n",
      "cooc_file_path_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/cooc_tf.txt\n",
      "cooc_file_path_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/cooc_df.txt\n",
      "ppmi_dict_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/ppmi_tf.txt\n",
      "ppmi_dict_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/ppmi_df.txt\n",
      "Parsing text collection...    1%\u001b[5D   2%\u001b[5D   3%\u001b[5D   4%\u001b[5D   5%\u001b[5D   6%\u001b[5D   7%\u001b[5D   8%\u001b[5D   9%\u001b[5D  10%\u001b[5D  11%\u001b[5D  12%\u001b[5D  13%\u001b[5D  14%\u001b[5D  15%\u001b[5D  16%\u001b[5D  17%\u001b[5D  18%\u001b[5D  19%\u001b[5D  20%\u001b[5D  21%\u001b[5D  22%\u001b[5D  23%\u001b[5D  24%\u001b[5D  25%\u001b[5D  26%\u001b[5D  27%\u001b[5D  28%\u001b[5D  29%\u001b[5D  30%\u001b[5D  31%\u001b[5D  32%\u001b[5D  33%\u001b[5D  34%\u001b[5D  35%\u001b[5D  36%\u001b[5D  37%\u001b[5D  38%\u001b[5D  39%\u001b[5D  40%\u001b[5D  41%\u001b[5D  42%\u001b[5D  43%\u001b[5D  44%\u001b[5D  45%\u001b[5D  46%\u001b[5D  47%\u001b[5D  48%\u001b[5D  49%\u001b[5D  50%\u001b[5D  51%\u001b[5D  52%\u001b[5D  53%\u001b[5D  54%\u001b[5D  55%\u001b[5D  56%\u001b[5D  57%\u001b[5D  58%\u001b[5D  59%\u001b[5D  60%\u001b[5D  61%\u001b[5D  62%\u001b[5D  63%\u001b[5D  64%\u001b[5D  65%\u001b[5D  66%\u001b[5D  67%\u001b[5D  68%\u001b[5D  69%\u001b[5D  70%\u001b[5D  71%\u001b[5D  72%\u001b[5D  73%\u001b[5D  74%\u001b[5D  75%\u001b[5D  76%\u001b[5D  77%\u001b[5D  78%\u001b[5D  79%\u001b[5D  80%\u001b[5D  81%\u001b[5D  82%\u001b[5D  83%\u001b[5D  84%\u001b[5D  85%\u001b[5D  86%\u001b[5D  87%\u001b[5D  88%\u001b[5D  89%\u001b[5D  90%\u001b[5D  91%\u001b[5D  92%\u001b[5D  93%\u001b[5D  94%\u001b[5D  95%\u001b[5D  96%\u001b[5D  97%\u001b[5D  98%\u001b[5D  99%\u001b[5D 100%\u001b[5D\n",
      "Merging co-occurrence batches. Stage 1: parallel agglomerative merge\n",
      "Merging co-occurrence batches. Stage 2: sequential merge\n",
      "Calculating pPMI\n",
      "OK.  \n",
      "23 batches created with total of 11299 items, and 60071 words in the dictionary; NNZ = 909128, average token weight is 1.46467\n",
      "time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "prepare_all(NG_DATASET, ng_column_name, dataset_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = NG_DATASET\n",
    "\n",
    "SAVE_DATASET_PATH = os.path.join(DATASET_PATH, 'processed_dataset.csv')\n",
    "BATCHES_DIR = os.path.join(DATASET_PATH, 'batches')\n",
    "WV_PATH = os.path.join(DATASET_PATH, 'wv.txt')\n",
    "DOCUMENTS_TO_BATCH_PATH = os.path.join(DATASET_PATH, 'dataset.csv')\n",
    "DICTIONARY_PATH = os.path.join(DATASET_PATH, 'dictionary.txt')\n",
    "VOCAB_PATH = os.path.join(DATASET_PATH, 'vocab.txt')\n",
    "COOC_DICTIONARY_PATH = os.path.join(DATASET_PATH, 'cooc_dictionary.txt')\n",
    "cooc_file_path_tf = os.path.join(DATASET_PATH, 'cooc_tf.txt')\n",
    "cooc_file_path_df = os.path.join(DATASET_PATH, 'cooc_df.txt')\n",
    "ppmi_dict_tf = os.path.join(DATASET_PATH,'ppmi_tf.txt')\n",
    "ppmi_dict_df = os.path.join(DATASET_PATH, 'ppmi_df.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches\t\t     cooc_tf.txt     ppmi_df.txt\t    vocab.txt\n",
      "cooc_df.txt\t     dataset.csv     ppmi_tf.txt\t    wv.txt\n",
      "cooc_dictionary.txt  dictionary.txt  processed_dataset.csv\n",
      "time: 407 ms\n"
     ]
    }
   ],
   "source": [
    "! ls $NG_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! head -5 /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/20newsgroups_sample/test_set_data_voc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FOLDER = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/test_folder'\n",
    "TEST_DATASET_PATH = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/20newsgroups_sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/20newsgroups_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_WV_PATH = os.path.join(TEST_DATASET_PATH, 'test_set_data_voc.txt')\n",
    "TEST_VOCAB_PATH = os.path.join(TEST_DATASET_PATH, 'vocab.txt')\n",
    "test_cooc_file_path_tf = os.path.join(TEST_FOLDER, 'cooc_tf.txt')\n",
    "test_cooc_file_path_df = os.path.join(TEST_FOLDER, 'cooc_df.txt')\n",
    "test_ppmi_dict_tf = os.path.join(TEST_FOLDER, 'ppmi_tf.txt')\n",
    "test_ppmi_dict_df = os.path.join(TEST_FOLDER, 'ppmi_df.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! head -5 $TEST_WV_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bigartm -c $TEST_WV_PATH -v $TEST_VOCAB_PATH --cooc-window 10 --write-cooc-tf $test_cooc_file_path_tf --write-cooc-df $test_cooc_file_path_df --write-ppmi-tf $test_ppmi_dict_tf --write-ppmi-df $test_ppmi_dict_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $TEST_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bigartm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/wv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r bigartm.WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_food = pd.read_csv('/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/amazon_food/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /mnt/ess_storage/DN_1/archive/jupyter-notebooks-133/mailru/kh-mruml/notebooks/tm/Quality_Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bigartm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
