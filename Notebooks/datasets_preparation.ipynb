{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymystem3\n",
      "  Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from pymystem3) (2.19.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->pymystem3) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->pymystem3) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->pymystem3) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->pymystem3) (2.7)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n",
      "time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "! pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 3.69 ms\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pymystem3\n",
    "from pyspark.sql.types import BooleanType, IntegerType, StringType, FloatType, ArrayType\n",
    "from pyspark.sql.functions import dayofyear, weekofyear, to_date, concat_ws, collect_list, year, udf, col, size, countDistinct, lit, sum, expr \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "import re\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import artm\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyarrow import parquet\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 308 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "full_datasets_path = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 330 µs\n"
     ]
    }
   ],
   "source": [
    "dataset_names = [\"20newsgroups\",\n",
    "                 \"amazon_food\",\n",
    "                 \"hotel-reviews\",\n",
    "                 \"lenta_ru\",\n",
    "                 \"ads_data\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20newsgroups\t     NIPS\t  hotel-reviews  synthetic\n",
      "20newsgroups_sample  amazon_food  lenta_ru\t test_set_data\n",
      "time: 402 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20newsgroups  ads_data\tamazon_food  hotel-reviews  lenta_ru\n",
      "time: 434 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.69 s\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/socks.py:58: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Callable\n",
      "Installing mystem to /home/jovyan/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "# dataset cleaning \n",
    "r_num = re.compile(r'([0-9]+)')\n",
    "r_punct = re.compile(r'[.\"\\[\\]/,()!?;:*#|\\\\%^$&{}~_`=-@]')\n",
    "r_white_space = re.compile(r'\\s{2,}')\n",
    "r_words = re.compile(r'\\W+')\n",
    "re1 = re.compile(r'  +')\n",
    "url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "r_rus = re.compile(r'[а-яА-Я]\\w+')\n",
    "r_html = re.compile(r'(\\<[^>]*\\>)')\n",
    "r_email = re.compile(r\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\")\n",
    "\n",
    "stop = stopwords.words(\"russian\") + ['который', 'это', 'весьма', 'вполне', 'наверное']\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "def process_punkt(text):\n",
    "    text = r_punct.sub(\" \", text)\n",
    "    text = r_num.sub(\" \", text)\n",
    "    text = r_white_space.sub(\" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def new_html(text):\n",
    "    text = r_html.sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_text_ru(text):\n",
    "\n",
    "    try:\n",
    "        text = new_html(text)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "    text = text.lower()\n",
    "    text = process_punkt(text)\n",
    "    text = re.findall(r_rus, text)\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    try:\n",
    "        tokens = r_words.split(text)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "    tokens = (x for x in tokens if len(x) >= 2 and not x.isdigit())\n",
    "    text = ' '.join(tokens)\n",
    "    tokens = m.lemmatize(text)\n",
    "    tokens = (x for x in tokens if x not in stop)\n",
    "    tokens = (x for x in tokens if x.isalpha())\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "def lemmatize_text_en(line):\n",
    "    try:\n",
    "        line = new_html(line)\n",
    "    except:\n",
    "        return ''\n",
    "    lmtzr = WordNetLemmatizer()    \n",
    "    stop = stopwords.words('english')\n",
    "    text_token = CountVectorizer().build_tokenizer()(line.lower())\n",
    "    text_token = (x for x in text_token if len(x) > 2 and not x.isdigit())\n",
    "    text_rmstop = (i for i in text_token if i not in stop)\n",
    "    text_rmstop = (x for x in text_rmstop if x.isalpha())\n",
    "    text_stem = ' '.join([lmtzr.lemmatize(w) for w in text_rmstop])\n",
    "    #stext_stem = remove_more_html(text_stem)\n",
    "    return text_stem\n",
    "    \n",
    "\n",
    "def text_to_tokens(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def remove_more_html(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('<', ' ').replace('>', ' ').replace('#36;', '$').replace(\n",
    "        '\\\\n', \"\\n\").replace('quot;', \"'\").replace('<br />', \"\\n\").replace(\n",
    "        '\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ').replace('img', ' ').replace('class', ' ').replace(\n",
    "        'src', ' ').replace('alt', ' ').replace('email', ' ').replace('icq', ' ').replace(\n",
    "        'href', ' ').replace('mem', ' ').replace('link', ' ').replace('mention', ' ').replace(\n",
    "        'onclick', ' ').replace('icq', ' ').replace('onmouseover', ' ').replace(\n",
    "        'local', ' ').replace('key', ' ').replace('target', ' ').replace('amp', ' ').replace(\n",
    "        'section', ' ').replace('search', ' ').replace('css', ' ').replace('style', ' ').replace(\n",
    "        'cc', ' ').replace('text', ' ').replace(\"img\", ' ').replace(\"expand\", ' ').replace(\n",
    "        \"text\", ' ').replace('\\n', ' ').replace('dnum', ' ').replace('xnum', ' ').replace(\n",
    "        'nnum', ' ').replace(\"ubuntu\", ' ').replace('server', ' ').replace('port', ' ').replace('nntp', ' ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def clear_url(text):\n",
    "    return re.sub(url_pattern, ' ', text)\n",
    "\n",
    "\n",
    "def tokens_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def get_rid_of_num(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [i for i in tokens if i != 'num']\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.25 ms\n"
     ]
    }
   ],
   "source": [
    "def clean_dataset(dataset_path, save_dataset_path, column_name, language):\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    p = mp.Pool(mp.cpu_count())\n",
    "    if language == \"ru\":\n",
    "        dataset['processed_text'] = p.map(lemmatize_text_ru, dataset[column_name])\n",
    "    if language == 'en':\n",
    "        dataset['processed_text'] = p.map(lemmatize_text_en, dataset[column_name])\n",
    "    dataset['tokens_count'] = p.map(tokens_count, dataset['processed_text'])\n",
    "    \n",
    "    print('Initial dataset shape: ', dataset.shape)\n",
    "    dataset = dataset[dataset['tokens_count'] >= 3]\n",
    "    print('Filtered dataset shape: ', dataset.shape)\n",
    "    \n",
    "    dataset.to_csv(save_dataset_path, index=None)\n",
    "    print('Dataset is saved')\n",
    "    \n",
    "    return\n",
    "    \n",
    "\n",
    "def get_words_dict(text, stop_list):\n",
    "    \n",
    "    all_words = text\n",
    "    words = sorted(set(all_words)-stop_list)\n",
    "    \n",
    "    return {w: all_words.count(w) for w in words}\n",
    "\n",
    "def return_string_part(name_type, text):\n",
    "    \n",
    "    tokens = text.split()\n",
    "    tokens = [item for item in tokens if item != '']\n",
    "    tokens_dict = get_words_dict(tokens, set())\n",
    "    \n",
    "    return \" |\" + name_type + ' ' + ' '.join(['{}:{}'.format(k, v) for k, v in tokens_dict.items()])\n",
    "\n",
    "def prepare_voc(batches_dir, vw_path, data_path, column_name='processed_text'):\n",
    "    \n",
    "    print('Starting...')\n",
    "    with open(vw_path, 'w', encoding='utf8') as ofile:\n",
    "        num_parts = 0\n",
    "        try:\n",
    "            for file in os.listdir(data_path):\n",
    "                if file.startswith('part'):\n",
    "                    print('part_{}'.format(num_parts), end='\\r')\n",
    "                    if file.split('.')[-1] == 'csv':\n",
    "                        part=pd.read_csv(os.path.join(data_path, file))\n",
    "                    else:\n",
    "                        part = pd.read_parquet(os.path.join(data_path, file))\n",
    "                    part_processed = part[column_name].tolist()\n",
    "                    for text in part_processed:\n",
    "                        result = return_string_part('@default_class', text)\n",
    "                        ofile.write(result + '\\n')\n",
    "                    num_parts += 1\n",
    "\n",
    "        except NotADirectoryError:\n",
    "            print('part 1/1')\n",
    "            part = pd.read_csv(data_path)\n",
    "            part_processed = part[column_name].tolist()\n",
    "            for text in part_processed:\n",
    "                result = return_string_part('@default_class', text)\n",
    "                ofile.write(result + '\\n')\n",
    "\n",
    "    print(' batches {} \\n vocabulary {} \\n are ready'.format(batches_dir, vw_path))\n",
    "    \n",
    "def prepare_batch_vectorizer(batches_dir, vw_path, data_path, column_name='processed_text'):\n",
    "#     if not glob.glob(os.path.join(batches_dir, \"*\")):\n",
    "    prepare_voc(batches_dir, vw_path, data_path, column_name=column_name)\n",
    "    batch_vectorizer = artm.BatchVectorizer(data_path=vw_path,\n",
    "        data_format=\"vowpal_wabbit\", \n",
    "        target_folder=batches_dir,\n",
    "    batch_size=100)\n",
    "#     else:\n",
    "#         batch_vectorizer = artm.BatchVectorizer(data_path=batches_dir, data_format='batches')\n",
    "        \n",
    "    return batch_vectorizer\n",
    "\n",
    "def vocab_preparation(VOCAB_PATH, DICTIONARY_PATH):\n",
    "    \n",
    "    if not os.path.exists(VOCAB_PATH):\n",
    "        with open(DICTIONARY_PATH, 'r') as dictionary_file:\n",
    "            with open(VOCAB_PATH, 'w') as vocab_file:\n",
    "                dictionary_file.readline()\n",
    "                dictionary_file.readline()\n",
    "                for line in dictionary_file:\n",
    "                    elems = re.split(', ', line)\n",
    "                    vocab_file.write(' '.join(elems[:2]) + '\\n')\n",
    "\n",
    "                    \n",
    "def prepearing_cooc_dict(BATCHES_DIR, WV_PATH, VOCAB_PATH, COOC_DICTIONARY_PATH, \n",
    "                         cooc_file_path_tf, cooc_file_path_df, ppmi_dict_tf, ppmi_dict_df):\n",
    "    print(f'BATCHES_DIR: {BATCHES_DIR}')\n",
    "    print(f'WV_PATH: {WV_PATH}')\n",
    "    print(f'VOCAB_PATH: {VOCAB_PATH}')\n",
    "    print(f'COOC_DICTIONARY_PATH: {COOC_DICTIONARY_PATH}')\n",
    "    print(f'cooc_file_path_tf: {cooc_file_path_tf}')\n",
    "    print(f'cooc_file_path_df: {cooc_file_path_df}')\n",
    "    print(f'ppmi_dict_tf: {ppmi_dict_tf}')\n",
    "    print(f'ppmi_dict_df: {ppmi_dict_df}')\n",
    "    \n",
    "    ! bigartm -v $VOCAB_PATH -c $WV_PATH --cooc-window 10 --write-cooc-tf $cooc_file_path_tf --write-cooc-df $cooc_file_path_df --write-ppmi-tf $ppmi_dict_tf --write-ppmi-df $ppmi_dict_df\n",
    "    \n",
    "    cooc_dict = artm.Dictionary()\n",
    "    cooc_dict.gather(\n",
    "        data_path=BATCHES_DIR, \n",
    "        cooc_file_path=ppmi_dict_tf,\n",
    "        vocab_file_path=VOCAB_PATH,\n",
    "        symmetric_cooc_values=True)\n",
    "    cooc_dict.save_text(COOC_DICTIONARY_PATH)\n",
    "                    \n",
    "        \n",
    "def mutual_info_dict_preparation(fname):\n",
    "\n",
    "    tokens_dict = {}\n",
    "\n",
    "    with open(fname) as handle:\n",
    "        for ix, line in enumerate(handle):\n",
    "            list_of_words = line.strip().split()\n",
    "            word_1 = list_of_words[0]\n",
    "            for word_val in list_of_words[1:]:\n",
    "                word_2, value = word_val.split(':')\n",
    "                tokens_dict['{}_{}'.format(word_1, word_2)] = float(value)\n",
    "                tokens_dict['{}_{}'.format(word_2, word_1)] = float(value)\n",
    "    return tokens_dict\n",
    "        \n",
    "def prepare_all(DATASET_PATH, column_name, dataset_lang):\n",
    "    \n",
    "    SAVE_DATASET_PATH = os.path.join(DATASET_PATH, 'processed_dataset.csv')\n",
    "    BATCHES_DIR = os.path.join(DATASET_PATH, 'batches')\n",
    "    WV_PATH = os.path.join(DATASET_PATH, 'wv.txt')\n",
    "    DOCUMENTS_TO_BATCH_PATH = os.path.join(DATASET_PATH, 'dataset.csv')\n",
    "    DICTIONARY_PATH = os.path.join(DATASET_PATH, 'dictionary.txt')\n",
    "    VOCAB_PATH = os.path.join(DATASET_PATH, 'vocab.txt')\n",
    "    COOC_DICTIONARY_PATH = os.path.join(DATASET_PATH, 'cooc_dictionary.txt')\n",
    "    cooc_file_path_tf = os.path.join(DATASET_PATH, 'cooc_tf.txt')\n",
    "    cooc_file_path_df = os.path.join(DATASET_PATH, 'cooc_df.txt')\n",
    "    ppmi_dict_tf = os.path.join(DATASET_PATH,'ppmi_tf.txt')\n",
    "    ppmi_dict_df = os.path.join(DATASET_PATH, 'ppmi_df.txt')\n",
    "    MUTUAL_INFO_DICT_PATH = os.path.join(DATASET_PATH,'mutual_info_dict.pkl')\n",
    "    \n",
    "    clean_dataset(DOCUMENTS_TO_BATCH_PATH, SAVE_DATASET_PATH, column_name, dataset_lang)\n",
    "    \n",
    "    batch_vectorizer = prepare_batch_vectorizer(BATCHES_DIR, WV_PATH, SAVE_DATASET_PATH)\n",
    "    print('Batches are ready')\n",
    "    \n",
    "    my_dictionary = artm.Dictionary()\n",
    "    my_dictionary.gather(data_path=BATCHES_DIR, vocab_file_path=WV_PATH)\n",
    "    my_dictionary.filter(min_df=3, max_df=0.95, class_id='text')\n",
    "    my_dictionary.save_text(DICTIONARY_PATH)\n",
    "    print('Dictionary is ready')\n",
    "    \n",
    "    vocab_preparation(VOCAB_PATH, DICTIONARY_PATH)\n",
    "    prepearing_cooc_dict(BATCHES_DIR, WV_PATH, VOCAB_PATH, \n",
    "                         COOC_DICTIONARY_PATH, cooc_file_path_tf, \n",
    "                         cooc_file_path_df, ppmi_dict_tf, \n",
    "                         ppmi_dict_df)\n",
    "    \n",
    "    mutual_info_dict = mutual_info_dict_preparation(ppmi_dict_tf)\n",
    "    with open(MUTUAL_INFO_DICT_PATH, 'wb') as handle:\n",
    "        pickle.dump(mutual_info_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 Newsgroups full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 313 µs\n"
     ]
    }
   ],
   "source": [
    "NG_DATASET = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches\t\t     dataset.csv\t   ppmi_tf.txt\n",
      "cooc_df.txt\t     dictionary.txt\t   processed_dataset.csv\n",
      "cooc_dictionary.txt  mutual_info_dict.pkl  vocab.txt\n",
      "cooc_tf.txt\t     ppmi_df.txt\t   wv.txt\n",
      "time: 571 ms\n"
     ]
    }
   ],
   "source": [
    "! ls $NG_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 428 µs\n"
     ]
    }
   ],
   "source": [
    "ng_column_name = 'processed_text'\n",
    "dataset_lang = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape:  (11299, 5)\n",
      "Filtered dataset shape:  (11280, 5)\n",
      "Dataset is saved\n",
      "Starting...\n",
      "part 1/1\n",
      " batches /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/batches \n",
      " vocabulary /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/wv.txt \n",
      " are ready\n",
      "Batches are ready\n",
      "Dictionary is ready\n",
      "BATCHES_DIR: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/batches\n",
      "WV_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/wv.txt\n",
      "VOCAB_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/vocab.txt\n",
      "COOC_DICTIONARY_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/cooc_dictionary.txt\n",
      "cooc_file_path_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/cooc_tf.txt\n",
      "cooc_file_path_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/cooc_df.txt\n",
      "ppmi_dict_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/ppmi_tf.txt\n",
      "ppmi_dict_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/ppmi_df.txt\n",
      "Parsing text collection...    1%\u001b[5D   2%\u001b[5D   3%\u001b[5D   4%\u001b[5D   5%\u001b[5D   6%\u001b[5D   7%\u001b[5D   8%\u001b[5D   9%\u001b[5D  10%\u001b[5D  11%\u001b[5D  12%\u001b[5D  13%\u001b[5D  14%\u001b[5D  15%\u001b[5D  16%\u001b[5D  17%\u001b[5D  18%\u001b[5D  19%\u001b[5D  20%\u001b[5D  21%\u001b[5D  22%\u001b[5D  23%\u001b[5D  24%\u001b[5D  25%\u001b[5D  26%\u001b[5D  27%\u001b[5D  28%\u001b[5D  29%\u001b[5D  30%\u001b[5D  31%\u001b[5D  32%\u001b[5D  33%\u001b[5D  34%\u001b[5D  35%\u001b[5D  36%\u001b[5D  37%\u001b[5D  38%\u001b[5D  39%\u001b[5D  40%\u001b[5D  41%\u001b[5D  42%\u001b[5D  43%\u001b[5D  44%\u001b[5D  45%\u001b[5D  46%\u001b[5D  47%\u001b[5D  48%\u001b[5D  49%\u001b[5D  50%\u001b[5D  51%\u001b[5D  52%\u001b[5D  53%\u001b[5D  54%\u001b[5D  55%\u001b[5D  56%\u001b[5D  57%\u001b[5D  58%\u001b[5D  59%\u001b[5D  60%\u001b[5D  61%\u001b[5D  62%\u001b[5D  63%\u001b[5D  64%\u001b[5D  65%\u001b[5D  66%\u001b[5D  67%\u001b[5D  68%\u001b[5D  69%\u001b[5D  70%\u001b[5D  71%\u001b[5D  72%\u001b[5D  73%\u001b[5D  74%\u001b[5D  75%\u001b[5D  76%\u001b[5D  77%\u001b[5D  78%\u001b[5D  79%\u001b[5D  80%\u001b[5D  81%\u001b[5D  82%\u001b[5D  83%\u001b[5D  84%\u001b[5D  85%\u001b[5D  86%\u001b[5D  87%\u001b[5D  88%\u001b[5D  89%\u001b[5D  90%\u001b[5D  91%\u001b[5D  92%\u001b[5D  93%\u001b[5D  94%\u001b[5D  95%\u001b[5D  96%\u001b[5D  97%\u001b[5D  98%\u001b[5D  99%\u001b[5D 100%\u001b[5D\n",
      "Merging co-occurrence batches. Stage 1: parallel agglomerative merge\n",
      "Merging co-occurrence batches. Stage 2: sequential merge\n",
      "Calculating pPMI\n",
      "OK.  \n",
      "23 batches created with total of 11280 items, and 59750 words in the dictionary; NNZ = 905190, average token weight is 1.47101\n",
      "time: 48.4 s\n"
     ]
    }
   ],
   "source": [
    "prepare_all(NG_DATASET, ng_column_name, dataset_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews.csv\t\t      cooc_dictionary.txt   ppmi_df.txt\r\n",
      "Reviews_10000_lemmatized.csv  cooc_tf.txt\t    ppmi_tf.txt\r\n",
      "batches\t\t\t      dictionary.txt\t    test_set_data_voc.txt\r\n",
      "cooc_df.txt\t\t      mutual_info_dict.pkl  vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/amazon_food/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "amazon_food = pd.read_csv('/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/amazon_food/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 962 ms\n"
     ]
    }
   ],
   "source": [
    "! cp /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/amazon_food/Reviews.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 419 ms\n"
     ]
    }
   ],
   "source": [
    "! mv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/Reviews.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "amazon_food.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.1 ms\n"
     ]
    }
   ],
   "source": [
    "amazon_food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape:  (568454, 12)\n",
      "Filtered dataset shape:  (568423, 12)\n",
      "Dataset is saved\n",
      "Starting...\n",
      "part 1/1\n",
      " batches /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/batches \n",
      " vocabulary /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/wv.txt \n",
      " are ready\n",
      "Batches are ready\n",
      "Dictionary is ready\n",
      "BATCHES_DIR: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/batches\n",
      "WV_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/wv.txt\n",
      "VOCAB_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/vocab.txt\n",
      "COOC_DICTIONARY_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/cooc_dictionary.txt\n",
      "cooc_file_path_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/cooc_tf.txt\n",
      "cooc_file_path_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/cooc_df.txt\n",
      "ppmi_dict_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/ppmi_tf.txt\n",
      "ppmi_dict_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/ppmi_df.txt\n",
      "Parsing text collection...    1%\u001b[5D   2%\u001b[5D   3%\u001b[5D   4%\u001b[5D   5%\u001b[5D   6%\u001b[5D   7%\u001b[5D   8%\u001b[5D   9%\u001b[5D  10%\u001b[5D  11%\u001b[5D  12%\u001b[5D  13%\u001b[5D  14%\u001b[5D  15%\u001b[5D  16%\u001b[5D  17%\u001b[5D  18%\u001b[5D  19%\u001b[5D  20%\u001b[5D  21%\u001b[5D  22%\u001b[5D  23%\u001b[5D  24%\u001b[5D  25%\u001b[5D  26%\u001b[5D  27%\u001b[5D  28%\u001b[5D  29%\u001b[5D  30%\u001b[5D  31%\u001b[5D  32%\u001b[5D  33%\u001b[5D  34%\u001b[5D  35%\u001b[5D  36%\u001b[5D  37%\u001b[5D  38%\u001b[5D  39%\u001b[5D  40%\u001b[5D  41%\u001b[5D  42%\u001b[5D  43%\u001b[5D  44%\u001b[5D  45%\u001b[5D  46%\u001b[5D  47%\u001b[5D  48%\u001b[5D  49%\u001b[5D  50%\u001b[5D  51%\u001b[5D  52%\u001b[5D  53%\u001b[5D  54%\u001b[5D  55%\u001b[5D  56%\u001b[5D  57%\u001b[5D  58%\u001b[5D  59%\u001b[5D  60%\u001b[5D  61%\u001b[5D  62%\u001b[5D  63%\u001b[5D  64%\u001b[5D  65%\u001b[5D  66%\u001b[5D  67%\u001b[5D  68%\u001b[5D  69%\u001b[5D  70%\u001b[5D  71%\u001b[5D  72%\u001b[5D  73%\u001b[5D  74%\u001b[5D  75%\u001b[5D  76%\u001b[5D  77%\u001b[5D  78%\u001b[5D  79%\u001b[5D  80%\u001b[5D  81%\u001b[5D  82%\u001b[5D  83%\u001b[5D  84%\u001b[5D  85%\u001b[5D  86%\u001b[5D  87%\u001b[5D  88%\u001b[5D  89%\u001b[5D  90%\u001b[5D  91%\u001b[5D  92%\u001b[5D  93%\u001b[5D  94%\u001b[5D  95%\u001b[5D  96%\u001b[5D  97%\u001b[5D  98%\u001b[5D  99%\u001b[5D 100%\u001b[5D\n",
      "Merging co-occurrence batches. Stage 1: parallel agglomerative merge\n",
      "Merging co-occurrence batches. Stage 2: sequential merge\n",
      "Calculating pPMI\n",
      "OK.  \n",
      "1137 batches created with total of 568423 items, and 115233 words in the dictionary; NNZ = 18345722, average token weight is 1.20785\n",
      "time: 7min 53s\n"
     ]
    }
   ],
   "source": [
    "AMAZON_DATASET = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/amazon_food/'\n",
    "text_column_name = 'Text'\n",
    "dataset_lang = 'en'\n",
    "\n",
    "prepare_all(AMAZON_DATASET, text_column_name, dataset_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches\t\t     dataset.csv\t   ppmi_tf.txt\n",
      "cooc_df.txt\t     dictionary.txt\t   processed_dataset.csv\n",
      "cooc_dictionary.txt  mutual_info_dict.pkl  vocab.txt\n",
      "cooc_tf.txt\t     ppmi_df.txt\t   wv.txt\n",
      "time: 428 ms\n"
     ]
    }
   ],
   "source": [
    "! ls $AMAZON_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/hotel-reviews/Datafiniti_Hotel_Reviews.csv\n",
      "time: 431 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/hotel-reviews/Datafiniti_Hotel_Reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 905 ms\n"
     ]
    }
   ],
   "source": [
    "! cp /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/hotel-reviews/Datafiniti_Hotel_Reviews.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 446 ms\n"
     ]
    }
   ],
   "source": [
    "! mv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/Datafiniti_Hotel_Reviews.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/dataset.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.csv\n",
      "time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 354 ms\n"
     ]
    }
   ],
   "source": [
    "hotel_reviews = pd.read_csv('/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/hotel-reviews/Datafiniti_Hotel_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape:  (10000, 27)\n",
      "Filtered dataset shape:  (9877, 27)\n",
      "Dataset is saved\n",
      "Starting...\n",
      "part 1/1\n",
      " batches /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/batches \n",
      " vocabulary /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/wv.txt \n",
      " are ready\n",
      "Batches are ready\n",
      "Dictionary is ready\n",
      "BATCHES_DIR: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/batches\n",
      "WV_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/wv.txt\n",
      "VOCAB_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/vocab.txt\n",
      "COOC_DICTIONARY_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/cooc_dictionary.txt\n",
      "cooc_file_path_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/cooc_tf.txt\n",
      "cooc_file_path_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/cooc_df.txt\n",
      "ppmi_dict_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/ppmi_tf.txt\n",
      "ppmi_dict_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews/ppmi_df.txt\n",
      "Parsing text collection...    1%\u001b[5D   2%\u001b[5D   3%\u001b[5D   4%\u001b[5D   5%\u001b[5D   6%\u001b[5D   7%\u001b[5D   8%\u001b[5D   9%\u001b[5D  10%\u001b[5D  11%\u001b[5D  12%\u001b[5D  13%\u001b[5D  14%\u001b[5D  15%\u001b[5D  16%\u001b[5D  17%\u001b[5D  18%\u001b[5D  19%\u001b[5D  20%\u001b[5D  21%\u001b[5D  22%\u001b[5D  23%\u001b[5D  24%\u001b[5D  25%\u001b[5D  26%\u001b[5D  27%\u001b[5D  28%\u001b[5D  29%\u001b[5D  30%\u001b[5D  31%\u001b[5D  32%\u001b[5D  33%\u001b[5D  34%\u001b[5D  35%\u001b[5D  36%\u001b[5D  37%\u001b[5D  38%\u001b[5D  39%\u001b[5D  40%\u001b[5D  41%\u001b[5D  42%\u001b[5D  43%\u001b[5D  44%\u001b[5D  45%\u001b[5D  46%\u001b[5D  47%\u001b[5D  48%\u001b[5D  49%\u001b[5D  50%\u001b[5D  51%\u001b[5D  52%\u001b[5D  53%\u001b[5D  54%\u001b[5D  55%\u001b[5D  56%\u001b[5D  57%\u001b[5D  58%\u001b[5D  59%\u001b[5D  60%\u001b[5D  61%\u001b[5D  62%\u001b[5D  63%\u001b[5D  64%\u001b[5D  65%\u001b[5D  66%\u001b[5D  67%\u001b[5D  68%\u001b[5D  69%\u001b[5D  70%\u001b[5D  71%\u001b[5D  72%\u001b[5D  73%\u001b[5D  74%\u001b[5D  75%\u001b[5D  76%\u001b[5D  77%\u001b[5D  78%\u001b[5D  79%\u001b[5D  80%\u001b[5D  81%\u001b[5D  82%\u001b[5D  83%\u001b[5D  84%\u001b[5D  85%\u001b[5D  86%\u001b[5D  87%\u001b[5D  88%\u001b[5D  89%\u001b[5D  90%\u001b[5D  91%\u001b[5D  92%\u001b[5D  93%\u001b[5D  94%\u001b[5D  95%\u001b[5D  96%\u001b[5D  97%\u001b[5D  98%\u001b[5D  99%\u001b[5D 100%\u001b[5D\n",
      "Merging co-occurrence batches. Stage 1: parallel agglomerative merge\n",
      "Merging co-occurrence batches. Stage 2: sequential merge\n",
      "Calculating pPMI\n",
      "OK.  \n",
      "20 batches created with total of 9877 items, and 13763 words in the dictionary; NNZ = 265792, average token weight is 1.11408\n",
      "time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "hr_column_name = 'reviews.text'\n",
    "HR_DATASET = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/hotel-reviews'\n",
    "dataset_lang = 'en'\n",
    "\n",
    "prepare_all(HR_DATASET, hr_column_name, dataset_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lenta.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches\t\t\t\t\t  lenta-ru-news_sample_15000.csv\n",
      "cooc_df.txt\t\t\t\t  mutual_info_dict.pkl\n",
      "cooc_dictionary.txt\t\t\t  ppmi_df.txt\n",
      "cooc_tf.txt\t\t\t\t  ppmi_tf.txt\n",
      "dictionary.txt\t\t\t\t  test_set_data_voc.txt\n",
      "lenta-ru-news.csv\t\t\t  vocab.txt\n",
      "lenta-ru-news_sample_10000_processed.csv\n",
      "time: 540 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/lenta_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.28 s\n"
     ]
    }
   ],
   "source": [
    "! cp /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/lenta_ru/lenta-ru-news.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 517 ms\n"
     ]
    }
   ],
   "source": [
    "! mv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/lenta-ru-news.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.csv\n",
      "time: 503 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "lenta_data = pd.read_csv('/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/lenta_ru/lenta-ru-news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3417: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape:  (800975, 8)\n",
      "Filtered dataset shape:  (799602, 8)\n",
      "Dataset is saved\n",
      "Starting...\n",
      "part 1/1\n",
      " batches /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/batches \n",
      " vocabulary /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/wv.txt \n",
      " are ready\n",
      "Batches are ready\n",
      "Dictionary is ready\n",
      "BATCHES_DIR: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/batches\n",
      "WV_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/wv.txt\n",
      "VOCAB_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/vocab.txt\n",
      "COOC_DICTIONARY_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/cooc_dictionary.txt\n",
      "cooc_file_path_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/cooc_tf.txt\n",
      "cooc_file_path_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/cooc_df.txt\n",
      "ppmi_dict_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/ppmi_tf.txt\n",
      "ppmi_dict_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru/ppmi_df.txt\n",
      "Parsing text collection...    1%\u001b[5D   2%\u001b[5D   3%\u001b[5D   4%\u001b[5D   5%\u001b[5D   6%\u001b[5D   7%\u001b[5D   8%\u001b[5D   9%\u001b[5D  10%\u001b[5D  11%\u001b[5D  12%\u001b[5D  13%\u001b[5D  14%\u001b[5D  15%\u001b[5D  16%\u001b[5D  17%\u001b[5D  18%\u001b[5D  19%\u001b[5D  20%\u001b[5D  21%\u001b[5D  22%\u001b[5D  23%\u001b[5D  24%\u001b[5D  25%\u001b[5D  26%\u001b[5D  27%\u001b[5D  28%\u001b[5D  29%\u001b[5D  30%\u001b[5D  31%\u001b[5D  32%\u001b[5D  33%\u001b[5D  34%\u001b[5D  35%\u001b[5D  36%\u001b[5D  37%\u001b[5D  38%\u001b[5D  39%\u001b[5D  40%\u001b[5D  41%\u001b[5D  42%\u001b[5D  43%\u001b[5D  44%\u001b[5D  45%\u001b[5D  46%\u001b[5D  47%\u001b[5D  48%\u001b[5D  49%\u001b[5D  50%\u001b[5D  51%\u001b[5D  52%\u001b[5D  53%\u001b[5D  54%\u001b[5D  55%\u001b[5D  56%\u001b[5D  57%\u001b[5D  58%\u001b[5D  59%\u001b[5D  60%\u001b[5D  61%\u001b[5D  62%\u001b[5D  63%\u001b[5D  64%\u001b[5D  65%\u001b[5D  66%\u001b[5D  67%\u001b[5D  68%\u001b[5D  69%\u001b[5D  70%\u001b[5D  71%\u001b[5D  72%\u001b[5D  73%\u001b[5D  74%\u001b[5D  75%\u001b[5D  76%\u001b[5D  77%\u001b[5D  78%\u001b[5D  79%\u001b[5D  80%\u001b[5D  81%\u001b[5D  82%\u001b[5D  83%\u001b[5D  84%\u001b[5D  85%\u001b[5D  86%\u001b[5D  87%\u001b[5D  88%\u001b[5D  89%\u001b[5D  90%\u001b[5D  91%\u001b[5D  92%\u001b[5D  93%\u001b[5D  94%\u001b[5D  95%\u001b[5D  96%\u001b[5D  97%\u001b[5D  98%\u001b[5D  99%\u001b[5D 100%\u001b[5D\n",
      "Merging co-occurrence batches. Stage 1: parallel agglomerative merge\n",
      "Merging co-occurrence batches. Stage 2: sequential merge\n",
      "Calculating pPMI\n",
      "OK.  \n",
      "1600 batches created with total of 799602 items, and 425231 words in the dictionary; NNZ = 74722136, average token weight is 1.32804\n",
      "time: 44min 6s\n"
     ]
    }
   ],
   "source": [
    "text_column = 'text'\n",
    "lenta_dataset = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/lenta_ru'\n",
    "dataset_lang = 'ru'\n",
    "\n",
    "prepare_all(lenta_dataset, text_column, dataset_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ads dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banners_content_gz\tcooc_dictionary.txt   ppmi_df.txt\n",
      "banners_content_pd.csv\tcooc_tf.txt\t      ppmi_tf.txt\n",
      "batches\t\t\tdictionary.txt\t      test_set_data_voc.txt\n",
      "cooc_df.txt\t\tmutual_info_dict.pkl  vocab.txt\n",
      "time: 771 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/test_set_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "banners = pd.read_csv('/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/test_set_data/banners_content_pd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://autonew16.ru/catalog/tuning-aksessuari-...</td>\n",
       "      <td>Отзывы\\nДоставка и оплата\\nКонтакты\\nКак заказ...</td>\n",
       "      <td>Купить тюнинг на ВАЗ (Лада) 21099 — аксессуары...</td>\n",
       "      <td>отзыв доставка оплата контакт заказывать тюнин...</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://baza-creditov.ru/</td>\n",
       "      <td>Сумма кредита\\nот\\nдо\\n7000\\n200000 руб.\\nПроц...</td>\n",
       "      <td>База кредитов: займы и рассрочки</td>\n",
       "      <td>сумма кредит руб процентный ставка срок кредит...</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://dom.tv.of.by/</td>\n",
       "      <td>НОЖ кредитка в подарок\\nневозможно поцарапать\\...</td>\n",
       "      <td>Часы бизнес-класса DOM - невозможно поцарапать...</td>\n",
       "      <td>нож кредитка подарок невозможно поцарапать час...</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://e96.ru/kids/baby_cribs/detskie_krovatki...</td>\n",
       "      <td>Товары для кухни\\nВстраиваемая техника\\nТовары...</td>\n",
       "      <td>Детские кроватки для новорожденных — купить не...</td>\n",
       "      <td>товар кухня встраивать техника товар дом аудио...</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://ekotopas.ru/servisnoe-obsluzhivanie/kon...</td>\n",
       "      <td>Разовое обслуживание ТОПАС\\nКонсервация ТОПАС\\...</td>\n",
       "      <td>Консервация септика Топас на зиму 🚿 ekoTopas</td>\n",
       "      <td>разовый обслуживание топас консервация топас р...</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://autonew16.ru/catalog/tuning-aksessuari-...   \n",
       "1                           http://baza-creditov.ru/   \n",
       "2                               http://dom.tv.of.by/   \n",
       "3  http://e96.ru/kids/baby_cribs/detskie_krovatki...   \n",
       "4  http://ekotopas.ru/servisnoe-obsluzhivanie/kon...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Отзывы\\nДоставка и оплата\\nКонтакты\\nКак заказ...   \n",
       "1  Сумма кредита\\nот\\nдо\\n7000\\n200000 руб.\\nПроц...   \n",
       "2  НОЖ кредитка в подарок\\nневозможно поцарапать\\...   \n",
       "3  Товары для кухни\\nВстраиваемая техника\\nТовары...   \n",
       "4  Разовое обслуживание ТОПАС\\nКонсервация ТОПАС\\...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Купить тюнинг на ВАЗ (Лада) 21099 — аксессуары...   \n",
       "1                   База кредитов: займы и рассрочки   \n",
       "2  Часы бизнес-класса DOM - невозможно поцарапать...   \n",
       "3  Детские кроватки для новорожденных — купить не...   \n",
       "4       Консервация септика Топас на зиму 🚿 ekoTopas   \n",
       "\n",
       "                                      processed_text  tokens_count  \n",
       "0  отзыв доставка оплата контакт заказывать тюнин...           688  \n",
       "1  сумма кредит руб процентный ставка срок кредит...           424  \n",
       "2  нож кредитка подарок невозможно поцарапать час...           229  \n",
       "3  товар кухня встраивать техника товар дом аудио...           295  \n",
       "4  разовый обслуживание топас консервация топас р...           724  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.2 ms\n"
     ]
    }
   ],
   "source": [
    "banners.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 994 ms\n"
     ]
    }
   ],
   "source": [
    "! cp /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/test_set_data/banners_content_pd.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banners_content_pd.csv\n",
      "time: 697 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 724 ms\n"
     ]
    }
   ],
   "source": [
    "! mv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/banners_content_pd.csv /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape:  (10000, 5)\n",
      "Filtered dataset shape:  (10000, 5)\n",
      "Dataset is saved\n",
      "Starting...\n",
      "part 1/1\n",
      " batches /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/batches \n",
      " vocabulary /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/wv.txt \n",
      " are ready\n",
      "Batches are ready\n",
      "Dictionary is ready\n",
      "BATCHES_DIR: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/batches\n",
      "WV_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/wv.txt\n",
      "VOCAB_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/vocab.txt\n",
      "COOC_DICTIONARY_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/cooc_dictionary.txt\n",
      "cooc_file_path_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/cooc_tf.txt\n",
      "cooc_file_path_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/cooc_df.txt\n",
      "ppmi_dict_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/ppmi_tf.txt\n",
      "ppmi_dict_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data/ppmi_df.txt\n",
      "Parsing text collection...    1%\u001b[5D   2%\u001b[5D   3%\u001b[5D   4%\u001b[5D   5%\u001b[5D   6%\u001b[5D   7%\u001b[5D   8%\u001b[5D   9%\u001b[5D  10%\u001b[5D  11%\u001b[5D  12%\u001b[5D  13%\u001b[5D  14%\u001b[5D  15%\u001b[5D  16%\u001b[5D  17%\u001b[5D  18%\u001b[5D  19%\u001b[5D  20%\u001b[5D  21%\u001b[5D  22%\u001b[5D  23%\u001b[5D  24%\u001b[5D  25%\u001b[5D  26%\u001b[5D  27%\u001b[5D  28%\u001b[5D  29%\u001b[5D  30%\u001b[5D  31%\u001b[5D  32%\u001b[5D  33%\u001b[5D  34%\u001b[5D  35%\u001b[5D  36%\u001b[5D  37%\u001b[5D  38%\u001b[5D  39%\u001b[5D  40%\u001b[5D  41%\u001b[5D  42%\u001b[5D  43%\u001b[5D  44%\u001b[5D  45%\u001b[5D  46%\u001b[5D  47%\u001b[5D  48%\u001b[5D  49%\u001b[5D  50%\u001b[5D  51%\u001b[5D  52%\u001b[5D  53%\u001b[5D  54%\u001b[5D  55%\u001b[5D  56%\u001b[5D  57%\u001b[5D  58%\u001b[5D  59%\u001b[5D  60%\u001b[5D  61%\u001b[5D  62%\u001b[5D  63%\u001b[5D  64%\u001b[5D  65%\u001b[5D  66%\u001b[5D  67%\u001b[5D  68%\u001b[5D  69%\u001b[5D  70%\u001b[5D  71%\u001b[5D  72%\u001b[5D  73%\u001b[5D  74%\u001b[5D  75%\u001b[5D  76%\u001b[5D  77%\u001b[5D  78%\u001b[5D  79%\u001b[5D  80%\u001b[5D  81%\u001b[5D  82%\u001b[5D  83%\u001b[5D  84%\u001b[5D  85%\u001b[5D  86%\u001b[5D  87%\u001b[5D  88%\u001b[5D  89%\u001b[5D  90%\u001b[5D  91%\u001b[5D  92%\u001b[5D  93%\u001b[5D  94%\u001b[5D  95%\u001b[5D  96%\u001b[5D  97%\u001b[5D  98%\u001b[5D  99%\u001b[5D 100%\u001b[5D\n",
      "Merging co-occurrence batches. Stage 1: parallel agglomerative merge\n",
      "Merging co-occurrence batches. Stage 2: sequential merge\n",
      "Calculating pPMI\n",
      "OK.  \n",
      "20 batches created with total of 10000 items, and 72488 words in the dictionary; NNZ = 1703874, average token weight is 1.92227\n",
      "time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "text_column = 'processed_text'\n",
    "ads_dataset = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/ads_data'\n",
    "dataset_lang = 'ru'\n",
    "\n",
    "prepare_all(ads_dataset, text_column, dataset_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FOLDER = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/test_folder'\n",
    "TEST_DATASET_PATH = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/20newsgroups_sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/20newsgroups_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_WV_PATH = os.path.join(TEST_DATASET_PATH, 'test_set_data_voc.txt')\n",
    "TEST_VOCAB_PATH = os.path.join(TEST_DATASET_PATH, 'vocab.txt')\n",
    "test_cooc_file_path_tf = os.path.join(TEST_FOLDER, 'cooc_tf.txt')\n",
    "test_cooc_file_path_df = os.path.join(TEST_FOLDER, 'cooc_df.txt')\n",
    "test_ppmi_dict_tf = os.path.join(TEST_FOLDER, 'ppmi_tf.txt')\n",
    "test_ppmi_dict_df = os.path.join(TEST_FOLDER, 'ppmi_df.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! head -5 $TEST_WV_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bigartm -c $TEST_WV_PATH -v $TEST_VOCAB_PATH --cooc-window 10 --write-cooc-tf $test_cooc_file_path_tf --write-cooc-df $test_cooc_file_path_df --write-ppmi-tf $test_ppmi_dict_tf --write-ppmi-df $test_ppmi_dict_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls $TEST_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bigartm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups/wv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r bigartm.WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_food = pd.read_csv('/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets/amazon_food/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /mnt/ess_storage/DN_1/archive/jupyter-notebooks-133/mailru/kh-mruml/notebooks/tm/Quality_Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bigartm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogues datasets preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIALOGUE_DATA_PATH = \"/mnt/ess_storage/DN_1/storage/qa-system-research/ClickHouseChat/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 474 µs\n"
     ]
    }
   ],
   "source": [
    "clickhouse_data = os.path.join(MAIN_DIALOGUE_DATA_PATH, \"clickhouse_ru.combined_messages_by_author_and_time.json\")\n",
    "python_data = os.path.join(MAIN_DIALOGUE_DATA_PATH, \"ru_python.combined_messages_by_author_and_time.json\")\n",
    "kino_data = os.path.join(MAIN_DIALOGUE_DATA_PATH, \"kinota1k.combined_messages_by_author_and_time.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reply_to_msg_id</th>\n",
       "      <th>from_id</th>\n",
       "      <th>message</th>\n",
       "      <th>is_joint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1083186</td>\n",
       "      <td>2019-12-12 17:02:31</td>\n",
       "      <td>None</td>\n",
       "      <td>541532850.0</td>\n",
       "      <td>А, ясно, просто в venv его не установил. Спасибо</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1083185</td>\n",
       "      <td>2019-12-12 17:00:38</td>\n",
       "      <td>None</td>\n",
       "      <td>460960819.0</td>\n",
       "      <td>Привет. Есть кто подключал прокси с авторизаци...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1083184</td>\n",
       "      <td>2019-12-12 16:58:32</td>\n",
       "      <td>1083183</td>\n",
       "      <td>164184279.0</td>\n",
       "      <td>В том, что нет модуля</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1083183</td>\n",
       "      <td>2019-12-12 16:57:27</td>\n",
       "      <td>None</td>\n",
       "      <td>541532850.0</td>\n",
       "      <td>'''\\nimport requests\\nimport pprint\\n\\ndef get...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1083182</td>\n",
       "      <td>2019-12-12 16:56:30</td>\n",
       "      <td>None</td>\n",
       "      <td>771096498.0</td>\n",
       "      <td>Добро пожаловать, @lolervt!\\n\\nНе забудь ознак...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852493</th>\n",
       "      <td>7</td>\n",
       "      <td>2015-12-18 14:00:20</td>\n",
       "      <td>None</td>\n",
       "      <td>462722.0</td>\n",
       "      <td>новый линк\\nhttps://telegram.me/joinchat/AAcPg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852494</th>\n",
       "      <td>6</td>\n",
       "      <td>2015-12-18 14:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>62240365.0</td>\n",
       "      <td>а чего мы так ждали 200 если могли вручную это...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852495</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-12-18 13:59:44</td>\n",
       "      <td>None</td>\n",
       "      <td>63210888.0</td>\n",
       "      <td>теперь игра \"найди приглашательный линк\" ;)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852496</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-12-18 13:59:16</td>\n",
       "      <td>None</td>\n",
       "      <td>63210888.0</td>\n",
       "      <td>Ну теперь заживём :)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852497</th>\n",
       "      <td>2__3</td>\n",
       "      <td>2015-12-18 13:59:10</td>\n",
       "      <td>None</td>\n",
       "      <td>109400450.0</td>\n",
       "      <td>Цоа [JOIN] Ура</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>852498 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                date reply_to_msg_id      from_id  \\\n",
       "0       1083186 2019-12-12 17:02:31            None  541532850.0   \n",
       "1       1083185 2019-12-12 17:00:38            None  460960819.0   \n",
       "2       1083184 2019-12-12 16:58:32         1083183  164184279.0   \n",
       "3       1083183 2019-12-12 16:57:27            None  541532850.0   \n",
       "4       1083182 2019-12-12 16:56:30            None  771096498.0   \n",
       "...         ...                 ...             ...          ...   \n",
       "852493        7 2015-12-18 14:00:20            None     462722.0   \n",
       "852494        6 2015-12-18 14:00:00            None   62240365.0   \n",
       "852495        5 2015-12-18 13:59:44            None   63210888.0   \n",
       "852496        4 2015-12-18 13:59:16            None   63210888.0   \n",
       "852497     2__3 2015-12-18 13:59:10            None  109400450.0   \n",
       "\n",
       "                                                  message  is_joint  \n",
       "0        А, ясно, просто в venv его не установил. Спасибо         0  \n",
       "1       Привет. Есть кто подключал прокси с авторизаци...         0  \n",
       "2                                   В том, что нет модуля         0  \n",
       "3       '''\\nimport requests\\nimport pprint\\n\\ndef get...         0  \n",
       "4       Добро пожаловать, @lolervt!\\n\\nНе забудь ознак...         0  \n",
       "...                                                   ...       ...  \n",
       "852493  новый линк\\nhttps://telegram.me/joinchat/AAcPg...         0  \n",
       "852494  а чего мы так ждали 200 если могли вручную это...         0  \n",
       "852495        теперь игра \"найди приглашательный линк\" ;)         0  \n",
       "852496                               Ну теперь заживём :)         0  \n",
       "852497                                     Цоа [JOIN] Ура         1  \n",
       "\n",
       "[852498 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "pd.read_json(python_data, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 612 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "clickhouse_full_folder = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/'\n",
    "python_full_folder = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/'\n",
    "kino_full_folder = '/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/’: File exists\n",
      "mkdir: cannot create directory ‘/mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/’: File exists\n",
      "time: 809 ms\n"
     ]
    }
   ],
   "source": [
    "! mkdir $clickhouse_full_folder\n",
    "! mkdir $python_full_folder\n",
    "! mkdir $kino_full_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.37 s\n"
     ]
    }
   ],
   "source": [
    "clickhouse_df = pd.read_json(clickhouse_data, lines=True)\n",
    "python_df = pd.read_json(python_data, lines=True)\n",
    "kino_df = pd.read_json(kino_data, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.24 s\n"
     ]
    }
   ],
   "source": [
    "clickhouse_df.to_csv(os.path.join(clickhouse_full_folder, \"dataset.csv\"), index=None)\n",
    "python_df.to_csv(os.path.join(python_full_folder, \"dataset.csv\"), index=None)\n",
    "kino_df.to_csv(os.path.join(kino_full_folder, \"dataset.csv\"), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clickhouse chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape:  (139852, 8)\n",
      "Filtered dataset shape:  (92677, 8)\n",
      "Dataset is saved\n",
      "Starting...\n",
      "part 1/1\n",
      " batches /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/batches \n",
      " vocabulary /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/wv.txt \n",
      " are ready\n",
      "Batches are ready\n",
      "Dictionary is ready\n",
      "BATCHES_DIR: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/batches\n",
      "WV_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/wv.txt\n",
      "VOCAB_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/vocab.txt\n",
      "COOC_DICTIONARY_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/cooc_dictionary.txt\n",
      "cooc_file_path_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/cooc_tf.txt\n",
      "cooc_file_path_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/cooc_df.txt\n",
      "ppmi_dict_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/ppmi_tf.txt\n",
      "ppmi_dict_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data/ppmi_df.txt\n",
      "Parsing text collection...    1%\u001b[5D   2%\u001b[5D   3%\u001b[5D   4%\u001b[5D   5%\u001b[5D   6%\u001b[5D   7%\u001b[5D   8%\u001b[5D   9%\u001b[5D  10%\u001b[5D  11%\u001b[5D  12%\u001b[5D  13%\u001b[5D  14%\u001b[5D  15%\u001b[5D  16%\u001b[5D  17%\u001b[5D  18%\u001b[5D  19%\u001b[5D  20%\u001b[5D  21%\u001b[5D  22%\u001b[5D  23%\u001b[5D  24%\u001b[5D  25%\u001b[5D  26%\u001b[5D  27%\u001b[5D  28%\u001b[5D  29%\u001b[5D  30%\u001b[5D  31%\u001b[5D  32%\u001b[5D  33%\u001b[5D  34%\u001b[5D  35%\u001b[5D  36%\u001b[5D  37%\u001b[5D  38%\u001b[5D  39%\u001b[5D  40%\u001b[5D  41%\u001b[5D  42%\u001b[5D  43%\u001b[5D  44%\u001b[5D  45%\u001b[5D  46%\u001b[5D  47%\u001b[5D  48%\u001b[5D  49%\u001b[5D  50%\u001b[5D  51%\u001b[5D  52%\u001b[5D  53%\u001b[5D  54%\u001b[5D  55%\u001b[5D  56%\u001b[5D  57%\u001b[5D  58%\u001b[5D  59%\u001b[5D  60%\u001b[5D  61%\u001b[5D  62%\u001b[5D  63%\u001b[5D  64%\u001b[5D  65%\u001b[5D  66%\u001b[5D  67%\u001b[5D  68%\u001b[5D  69%\u001b[5D  70%\u001b[5D  71%\u001b[5D  72%\u001b[5D  73%\u001b[5D  74%\u001b[5D  75%\u001b[5D  76%\u001b[5D  77%\u001b[5D  78%\u001b[5D  79%\u001b[5D  80%\u001b[5D  81%\u001b[5D  82%\u001b[5D  83%\u001b[5D  84%\u001b[5D  85%\u001b[5D  86%\u001b[5D  87%\u001b[5D  88%\u001b[5D  89%\u001b[5D  90%\u001b[5D  91%\u001b[5D  92%\u001b[5D  93%\u001b[5D  94%\u001b[5D  95%\u001b[5D  96%\u001b[5D  97%\u001b[5D  98%\u001b[5D  99%\u001b[5D 100%\u001b[5D\n",
      "Merging co-occurrence batches. Stage 1: parallel agglomerative merge\n",
      "Merging co-occurrence batches. Stage 2: sequential merge\n",
      "Calculating pPMI\n",
      "OK.  \n",
      "186 batches created with total of 92677 items, and 30318 words in the dictionary; NNZ = 822567, average token weight is 1.07936\n",
      "time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "text_column = 'message'\n",
    "main_path = clickhouse_full_folder\n",
    "dataset_lang = 'ru'\n",
    "\n",
    "prepare_all(main_path, text_column, dataset_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches\t\t     dataset.csv\t   ppmi_tf.txt\n",
      "cooc_df.txt\t     dictionary.txt\t   processed_dataset.csv\n",
      "cooc_dictionary.txt  mutual_info_dict.pkl  vocab.txt\n",
      "cooc_tf.txt\t     ppmi_df.txt\t   wv.txt\n",
      "time: 420 ms\n"
     ]
    }
   ],
   "source": [
    "! ls /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/clickhouse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape:  (852498, 8)\n",
      "Filtered dataset shape:  (456217, 8)\n",
      "Dataset is saved\n",
      "Starting...\n",
      "part 1/1\n",
      " batches /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/batches \n",
      " vocabulary /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/wv.txt \n",
      " are ready\n",
      "Batches are ready\n",
      "Dictionary is ready\n",
      "BATCHES_DIR: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/batches\n",
      "WV_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/wv.txt\n",
      "VOCAB_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/vocab.txt\n",
      "COOC_DICTIONARY_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/cooc_dictionary.txt\n",
      "cooc_file_path_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/cooc_tf.txt\n",
      "cooc_file_path_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/cooc_df.txt\n",
      "ppmi_dict_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/ppmi_tf.txt\n",
      "ppmi_dict_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/python_data/ppmi_df.txt\n",
      "Parsing text collection...    1%\u001b[5D   2%\u001b[5D   3%\u001b[5D   4%\u001b[5D   5%\u001b[5D   6%\u001b[5D   7%\u001b[5D   8%\u001b[5D   9%\u001b[5D  10%\u001b[5D  11%\u001b[5D  12%\u001b[5D  13%\u001b[5D  14%\u001b[5D  15%\u001b[5D  16%\u001b[5D  17%\u001b[5D  18%\u001b[5D  19%\u001b[5D  20%\u001b[5D  21%\u001b[5D  22%\u001b[5D  23%\u001b[5D  24%\u001b[5D  25%\u001b[5D  26%\u001b[5D  27%\u001b[5D  28%\u001b[5D  29%\u001b[5D  30%\u001b[5D  31%\u001b[5D  32%\u001b[5D  33%\u001b[5D  34%\u001b[5D  35%\u001b[5D  36%\u001b[5D  37%\u001b[5D  38%\u001b[5D  39%\u001b[5D  40%\u001b[5D  41%\u001b[5D  42%\u001b[5D  43%\u001b[5D  44%\u001b[5D  45%\u001b[5D  46%\u001b[5D  47%\u001b[5D  48%\u001b[5D  49%\u001b[5D  50%\u001b[5D  51%\u001b[5D  52%\u001b[5D  53%\u001b[5D  54%\u001b[5D  55%\u001b[5D  56%\u001b[5D  57%\u001b[5D  58%\u001b[5D  59%\u001b[5D  60%\u001b[5D  61%\u001b[5D  62%\u001b[5D  63%\u001b[5D  64%\u001b[5D  65%\u001b[5D  66%\u001b[5D  67%\u001b[5D  68%\u001b[5D  69%\u001b[5D  70%\u001b[5D  71%\u001b[5D  72%\u001b[5D  73%\u001b[5D  74%\u001b[5D  75%\u001b[5D  76%\u001b[5D  77%\u001b[5D  78%\u001b[5D  79%\u001b[5D  80%\u001b[5D  81%\u001b[5D  82%\u001b[5D  83%\u001b[5D  84%\u001b[5D  85%\u001b[5D  86%\u001b[5D  87%\u001b[5D  88%\u001b[5D  89%\u001b[5D  90%\u001b[5D  91%\u001b[5D  92%\u001b[5D  93%\u001b[5D  94%\u001b[5D  95%\u001b[5D  96%\u001b[5D  97%\u001b[5D  98%\u001b[5D  99%\u001b[5D 100%\u001b[5D\n",
      "Merging co-occurrence batches. Stage 1: parallel agglomerative merge\n",
      "Merging co-occurrence batches. Stage 2: sequential merge\n",
      "Calculating pPMI\n",
      "OK.  \n",
      "913 batches created with total of 456217 items, and 101095 words in the dictionary; NNZ = 3049853, average token weight is 1.04959\n",
      "time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "text_column = 'message'\n",
    "main_path = python_full_folder\n",
    "dataset_lang = 'ru'\n",
    "\n",
    "prepare_all(main_path, text_column, dataset_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kino chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape:  (62465, 8)\n",
      "Filtered dataset shape:  (36631, 8)\n",
      "Dataset is saved\n",
      "Starting...\n",
      "part 1/1\n",
      " batches /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/batches \n",
      " vocabulary /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/wv.txt \n",
      " are ready\n",
      "Batches are ready\n",
      "Dictionary is ready\n",
      "BATCHES_DIR: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/batches\n",
      "WV_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/wv.txt\n",
      "VOCAB_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/vocab.txt\n",
      "COOC_DICTIONARY_PATH: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/cooc_dictionary.txt\n",
      "cooc_file_path_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/cooc_tf.txt\n",
      "cooc_file_path_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/cooc_df.txt\n",
      "ppmi_dict_tf: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/ppmi_tf.txt\n",
      "ppmi_dict_df: /mnt/ess_storage/DN_1/storage/home/khodorchenko/GOTM/datasets_full/kino_data/ppmi_df.txt\n",
      "Parsing text collection...    1%\u001b[5D   2%\u001b[5D   3%\u001b[5D   4%\u001b[5D   5%\u001b[5D   6%\u001b[5D   7%\u001b[5D   8%\u001b[5D   9%\u001b[5D  10%\u001b[5D  11%\u001b[5D  12%\u001b[5D  13%\u001b[5D  14%\u001b[5D  15%\u001b[5D  16%\u001b[5D  17%\u001b[5D  18%\u001b[5D  19%\u001b[5D  20%\u001b[5D  21%\u001b[5D  22%\u001b[5D  23%\u001b[5D  24%\u001b[5D  25%\u001b[5D  26%\u001b[5D  27%\u001b[5D  28%\u001b[5D  29%\u001b[5D  30%\u001b[5D  31%\u001b[5D  32%\u001b[5D  33%\u001b[5D  34%\u001b[5D  35%\u001b[5D  36%\u001b[5D  37%\u001b[5D  38%\u001b[5D  39%\u001b[5D  40%\u001b[5D  41%\u001b[5D  42%\u001b[5D  43%\u001b[5D  44%\u001b[5D  45%\u001b[5D  46%\u001b[5D  47%\u001b[5D  48%\u001b[5D  49%\u001b[5D  50%\u001b[5D  51%\u001b[5D  52%\u001b[5D  53%\u001b[5D  54%\u001b[5D  55%\u001b[5D  56%\u001b[5D  57%\u001b[5D  58%\u001b[5D  59%\u001b[5D  60%\u001b[5D  61%\u001b[5D  62%\u001b[5D  63%\u001b[5D  64%\u001b[5D  65%\u001b[5D  66%\u001b[5D  67%\u001b[5D  68%\u001b[5D  69%\u001b[5D  70%\u001b[5D  71%\u001b[5D  72%\u001b[5D  73%\u001b[5D  74%\u001b[5D  75%\u001b[5D  76%\u001b[5D  77%\u001b[5D  78%\u001b[5D  79%\u001b[5D  80%\u001b[5D  81%\u001b[5D  82%\u001b[5D  83%\u001b[5D  84%\u001b[5D  85%\u001b[5D  86%\u001b[5D  87%\u001b[5D  88%\u001b[5D  89%\u001b[5D  90%\u001b[5D  91%\u001b[5D  92%\u001b[5D  93%\u001b[5D  94%\u001b[5D  95%\u001b[5D  96%\u001b[5D  97%\u001b[5D  98%\u001b[5D  99%\u001b[5D 100%\u001b[5D\n",
      "Merging co-occurrence batches. Stage 1: parallel agglomerative merge\n",
      "Merging co-occurrence batches. Stage 2: sequential merge\n",
      "Calculating pPMI\n",
      "OK.  \n",
      "74 batches created with total of 36631 items, and 36064 words in the dictionary; NNZ = 357140, average token weight is 1.06777\n",
      "time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "text_column = 'message'\n",
    "main_path = kino_full_folder\n",
    "dataset_lang = 'ru'\n",
    "\n",
    "prepare_all(main_path, text_column, dataset_lang)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
